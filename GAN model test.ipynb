{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 20:44:01.451285: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-29 20:44:01.456190: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-29 20:44:01.456209: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_256643/4085867847.py:8: read_data_sets (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:296: _maybe_download (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:299: _extract_images (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:304: _extract_labels (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:112: _dense_to_one_hot (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:328: _DataSet.__init__ (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "test_X = mnist.test.images\n",
    "test_Y = mnist.test.labels\n",
    "print(test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 784) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "validation_X = mnist.validation.images\n",
    "validation_Y = mnist.validation.labels\n",
    "print(validation_X.shape, validation_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_X = mnist.train.images\n",
    "train_Y = mnist.train.labels\n",
    "print(train_X.shape, train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0])  # 是一个包含784个元素且值在[0,1]之间的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化样本，下面是输出了训练集中前4个样本\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, sharex='all', sharey='all')\n",
    "ax = ax.flatten()\n",
    "for i in range(9):\n",
    "    img = train_X[i].reshape(28, 28)\n",
    "    # ax[i].imshow(img,cmap='Greys')\n",
    "    ax[i].imshow(img)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 32 #选择训练的批量大小\n",
    "X_dim = 784  #图片维度28*28\n",
    "z_dim = 64   #随机生成噪声的维度\n",
    "h_dim = 128\n",
    "lr = 1e-3 # learning rate\n",
    "d_steps = 3 \n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "##权重初始化函数方法\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)  #计算随机生成变量所服从的正太分布标准差\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)  #正态分布随机噪声\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x + 1e-8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution() ##解决在tf2版本下使用tf1的API\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim]) #input_real，定义输入图像占位符，784\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim]) #input_noise，定义随机噪声占位符，64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D_W1 = tf.Variable(xavier_init([X_dim + z_dim, h_dim])) #784+64，128， 定义判别器的权重和偏置项向量\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))##三层全连接的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义生成器的权重与偏置项。输入层为64个神经元且接受随机噪声\n",
    "# 输出层为64个神经元，并输出手写字体图片。生成网络根据原论文为三层全连接层\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "Q_W2 = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2 = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "theta_G = [Q_W1, Q_W2, Q_b1, Q_b2, P_W1, P_W2, P_b1, P_b2]\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "def sample_z(m, n):\n",
    "    # 从一个均匀分布[low,high)中随机采样，注意定义域是左闭右开，即包含low，不包含high.\n",
    "    # 返回值类型：ndarray类型，其形状和参数size中描述一致。\n",
    "    # ndarray类型，表示一个N维数组对象，其有一个shape（表维度大小）和dtype（说明数组数据类型的对象）\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    # 第一层先计算 y=X*Q_W1+Q_b1，然后投入激活函数计算h=ReLU(y),h为第二层神经网络的输出激活值\n",
    "    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1) # tf.matmul：矩阵乘法函数\n",
    "\n",
    "    # 以下计算第二层传播到第三层的激活结果，第三层的激活结果是含有784个元素的向量，该向量转化为28*28就可以表示图像\n",
    "    h = tf.matmul(h, Q_W2) + Q_b2 \n",
    "    return h\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
    "    h = tf.matmul(h, P_W2) + P_b2\n",
    "    return tf.nn.sigmoid(h)\n",
    "\n",
    "\n",
    "def D(X, z):\n",
    "    #判别器\n",
    "    inputs = tf.concat([X, z], axis=1)\n",
    "    h = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    return tf.nn.sigmoid(tf.matmul(h, D_W2) + D_b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 21:57:55.975656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.975822: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.975872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.975918: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.975966: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.976012: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.976056: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.976100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-26 21:57:55.976110: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-26 21:57:55.976339: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-26 21:57:55.986427: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; D_loss: 1.256; G_loss: 3.563\n",
      "Iter: 1000; D_loss: 0.004583; G_loss: 19.82\n",
      "Iter: 2000; D_loss: 0.007285; G_loss: 28.15\n",
      "Iter: 3000; D_loss: 0.003131; G_loss: 27.0\n",
      "Iter: 4000; D_loss: 0.04593; G_loss: 23.67\n",
      "Iter: 5000; D_loss: 0.02139; G_loss: 26.14\n",
      "Iter: 6000; D_loss: 0.02817; G_loss: 24.63\n",
      "Iter: 7000; D_loss: 0.08098; G_loss: 18.88\n",
      "Iter: 8000; D_loss: 0.06634; G_loss: 17.96\n",
      "Iter: 9000; D_loss: 0.32; G_loss: 18.05\n",
      "Iter: 10000; D_loss: 0.1348; G_loss: 17.86\n",
      "Iter: 11000; D_loss: 0.6028; G_loss: 15.39\n",
      "Iter: 12000; D_loss: 0.5099; G_loss: 13.06\n",
      "Iter: 13000; D_loss: 0.9303; G_loss: 13.68\n",
      "Iter: 14000; D_loss: 0.5183; G_loss: 11.35\n",
      "Iter: 15000; D_loss: 0.2619; G_loss: 12.6\n",
      "Iter: 16000; D_loss: 0.5407; G_loss: 9.215\n",
      "Iter: 17000; D_loss: 0.3337; G_loss: 10.12\n",
      "Iter: 18000; D_loss: 0.504; G_loss: 8.488\n",
      "Iter: 19000; D_loss: 0.4907; G_loss: 8.01\n",
      "Iter: 20000; D_loss: 0.6006; G_loss: 8.153\n",
      "Iter: 21000; D_loss: 0.5204; G_loss: 8.601\n",
      "Iter: 22000; D_loss: 0.4355; G_loss: 7.199\n",
      "Iter: 23000; D_loss: 0.541; G_loss: 9.333\n",
      "Iter: 24000; D_loss: 0.7219; G_loss: 6.295\n",
      "Iter: 25000; D_loss: 0.7376; G_loss: 5.964\n",
      "Iter: 26000; D_loss: 0.5945; G_loss: 6.689\n",
      "Iter: 27000; D_loss: 0.5143; G_loss: 6.396\n",
      "Iter: 28000; D_loss: 0.5685; G_loss: 6.361\n",
      "Iter: 29000; D_loss: 0.6675; G_loss: 6.516\n",
      "Iter: 30000; D_loss: 1.21; G_loss: 4.879\n",
      "Iter: 31000; D_loss: 0.6002; G_loss: 5.888\n",
      "Iter: 32000; D_loss: 0.9901; G_loss: 4.5\n",
      "Iter: 33000; D_loss: 0.6041; G_loss: 5.972\n",
      "Iter: 34000; D_loss: 0.9938; G_loss: 4.433\n",
      "Iter: 35000; D_loss: 0.3204; G_loss: 6.311\n",
      "Iter: 36000; D_loss: 1.127; G_loss: 4.52\n",
      "Iter: 37000; D_loss: 0.7113; G_loss: 5.847\n",
      "Iter: 38000; D_loss: 0.9077; G_loss: 4.655\n",
      "Iter: 39000; D_loss: 0.88; G_loss: 5.449\n",
      "Iter: 40000; D_loss: 0.9485; G_loss: 4.52\n",
      "Iter: 41000; D_loss: 0.8858; G_loss: 5.335\n",
      "Iter: 42000; D_loss: 0.7586; G_loss: 5.88\n",
      "Iter: 43000; D_loss: 0.83; G_loss: 4.996\n",
      "Iter: 44000; D_loss: 0.607; G_loss: 5.825\n",
      "Iter: 45000; D_loss: 0.631; G_loss: 6.298\n",
      "Iter: 46000; D_loss: 0.7498; G_loss: 4.693\n",
      "Iter: 47000; D_loss: 0.998; G_loss: 4.26\n",
      "Iter: 48000; D_loss: 0.6325; G_loss: 6.388\n",
      "Iter: 49000; D_loss: 0.727; G_loss: 5.403\n",
      "Iter: 50000; D_loss: 1.171; G_loss: 4.383\n",
      "Iter: 51000; D_loss: 0.8341; G_loss: 3.728\n",
      "Iter: 52000; D_loss: 0.7938; G_loss: 3.932\n",
      "Iter: 53000; D_loss: 0.6973; G_loss: 4.05\n",
      "Iter: 54000; D_loss: 0.9679; G_loss: 3.826\n",
      "Iter: 55000; D_loss: 0.9778; G_loss: 3.119\n",
      "Iter: 56000; D_loss: 0.7143; G_loss: 7.67\n",
      "Iter: 57000; D_loss: 0.5167; G_loss: 5.673\n",
      "Iter: 58000; D_loss: 0.7937; G_loss: 3.874\n",
      "Iter: 59000; D_loss: 0.8658; G_loss: 4.162\n",
      "Iter: 60000; D_loss: 0.7412; G_loss: 5.085\n",
      "Iter: 61000; D_loss: 0.7298; G_loss: 4.323\n",
      "Iter: 62000; D_loss: 0.9473; G_loss: 4.159\n",
      "Iter: 63000; D_loss: 0.7153; G_loss: 4.915\n",
      "Iter: 64000; D_loss: 1.038; G_loss: 4.386\n",
      "Iter: 65000; D_loss: 0.6305; G_loss: 4.495\n",
      "Iter: 66000; D_loss: 0.9204; G_loss: 4.251\n",
      "Iter: 67000; D_loss: 0.8936; G_loss: 3.957\n",
      "Iter: 68000; D_loss: 0.7757; G_loss: 4.654\n",
      "Iter: 69000; D_loss: 0.6571; G_loss: 5.062\n",
      "Iter: 70000; D_loss: 0.8883; G_loss: 4.011\n",
      "Iter: 71000; D_loss: 0.5213; G_loss: 4.874\n",
      "Iter: 72000; D_loss: 0.9294; G_loss: 4.343\n",
      "Iter: 73000; D_loss: 0.8958; G_loss: 4.957\n",
      "Iter: 74000; D_loss: 0.5819; G_loss: 4.643\n",
      "Iter: 75000; D_loss: 0.7166; G_loss: 4.618\n",
      "Iter: 76000; D_loss: 0.9452; G_loss: 3.913\n",
      "Iter: 77000; D_loss: 0.9136; G_loss: 4.77\n",
      "Iter: 78000; D_loss: 0.773; G_loss: 4.265\n",
      "Iter: 79000; D_loss: 0.8973; G_loss: 4.338\n",
      "Iter: 80000; D_loss: 0.9264; G_loss: 5.792\n",
      "Iter: 81000; D_loss: 0.7701; G_loss: 4.588\n",
      "Iter: 82000; D_loss: 0.9987; G_loss: 4.297\n",
      "Iter: 83000; D_loss: 0.6649; G_loss: 5.68\n",
      "Iter: 84000; D_loss: 0.6649; G_loss: 5.609\n",
      "Iter: 85000; D_loss: 0.8186; G_loss: 5.389\n",
      "Iter: 86000; D_loss: 0.9436; G_loss: 4.614\n",
      "Iter: 87000; D_loss: 0.8154; G_loss: 5.87\n",
      "Iter: 88000; D_loss: 0.9183; G_loss: 5.196\n",
      "Iter: 89000; D_loss: 0.7346; G_loss: 5.809\n",
      "Iter: 90000; D_loss: 0.5272; G_loss: 6.877\n",
      "Iter: 91000; D_loss: 0.4664; G_loss: 5.54\n",
      "Iter: 92000; D_loss: 0.7277; G_loss: 5.337\n",
      "Iter: 93000; D_loss: 0.8149; G_loss: 5.469\n",
      "Iter: 94000; D_loss: 0.6819; G_loss: 7.584\n",
      "Iter: 95000; D_loss: 0.8053; G_loss: 4.656\n",
      "Iter: 96000; D_loss: 0.5798; G_loss: 6.089\n",
      "Iter: 97000; D_loss: 0.5353; G_loss: 5.82\n",
      "Iter: 98000; D_loss: 0.7279; G_loss: 6.205\n",
      "Iter: 99000; D_loss: 0.7472; G_loss: 5.495\n",
      "Iter: 100000; D_loss: 0.4541; G_loss: 6.476\n",
      "Iter: 101000; D_loss: 0.5979; G_loss: 5.717\n",
      "Iter: 102000; D_loss: 0.7084; G_loss: 6.236\n",
      "Iter: 103000; D_loss: 0.6793; G_loss: 7.076\n",
      "Iter: 104000; D_loss: 0.4514; G_loss: 6.929\n",
      "Iter: 105000; D_loss: 0.6604; G_loss: 5.896\n",
      "Iter: 106000; D_loss: 0.653; G_loss: 5.913\n",
      "Iter: 107000; D_loss: 0.491; G_loss: 6.046\n",
      "Iter: 108000; D_loss: 0.6759; G_loss: 5.683\n",
      "Iter: 109000; D_loss: 0.6755; G_loss: 5.9\n",
      "Iter: 110000; D_loss: 0.5992; G_loss: 6.053\n",
      "Iter: 111000; D_loss: 0.9568; G_loss: 5.604\n",
      "Iter: 112000; D_loss: 0.6189; G_loss: 5.653\n",
      "Iter: 113000; D_loss: 0.8457; G_loss: 5.757\n",
      "Iter: 114000; D_loss: 0.2857; G_loss: 7.752\n",
      "Iter: 115000; D_loss: 0.6032; G_loss: 5.98\n",
      "Iter: 116000; D_loss: 0.5903; G_loss: 6.191\n",
      "Iter: 117000; D_loss: 0.5985; G_loss: 6.094\n",
      "Iter: 118000; D_loss: 0.7659; G_loss: 5.615\n",
      "Iter: 119000; D_loss: 0.5946; G_loss: 6.376\n",
      "Iter: 120000; D_loss: 0.619; G_loss: 6.506\n",
      "Iter: 121000; D_loss: 0.7388; G_loss: 6.028\n",
      "Iter: 122000; D_loss: 0.5923; G_loss: 7.314\n",
      "Iter: 123000; D_loss: 0.3868; G_loss: 6.323\n",
      "Iter: 124000; D_loss: 0.7213; G_loss: 6.417\n",
      "Iter: 125000; D_loss: 0.826; G_loss: 6.235\n",
      "Iter: 126000; D_loss: 0.6432; G_loss: 5.95\n",
      "Iter: 127000; D_loss: 0.423; G_loss: 7.188\n",
      "Iter: 128000; D_loss: 0.5967; G_loss: 6.032\n",
      "Iter: 129000; D_loss: 0.7995; G_loss: 6.006\n",
      "Iter: 130000; D_loss: 0.564; G_loss: 6.751\n",
      "Iter: 131000; D_loss: 0.7325; G_loss: 5.827\n",
      "Iter: 132000; D_loss: 0.6453; G_loss: 7.97\n",
      "Iter: 133000; D_loss: 0.7246; G_loss: 6.339\n",
      "Iter: 134000; D_loss: 0.6487; G_loss: 7.084\n",
      "Iter: 135000; D_loss: 0.5465; G_loss: 6.722\n",
      "Iter: 136000; D_loss: 0.5927; G_loss: 6.994\n",
      "Iter: 137000; D_loss: 0.9656; G_loss: 5.037\n",
      "Iter: 138000; D_loss: 0.8022; G_loss: 6.151\n",
      "Iter: 139000; D_loss: 0.7532; G_loss: 6.597\n",
      "Iter: 140000; D_loss: 0.6546; G_loss: 6.221\n",
      "Iter: 141000; D_loss: 0.5939; G_loss: 6.969\n",
      "Iter: 142000; D_loss: 0.3779; G_loss: 7.321\n",
      "Iter: 143000; D_loss: 0.8363; G_loss: 9.204\n",
      "Iter: 144000; D_loss: 0.4141; G_loss: 6.67\n",
      "Iter: 145000; D_loss: 0.6276; G_loss: 8.509\n",
      "Iter: 146000; D_loss: 0.4462; G_loss: 6.841\n",
      "Iter: 147000; D_loss: 0.8321; G_loss: 7.99\n",
      "Iter: 148000; D_loss: 0.3399; G_loss: 6.782\n",
      "Iter: 149000; D_loss: 0.5241; G_loss: 7.435\n",
      "Iter: 150000; D_loss: 0.547; G_loss: 7.202\n",
      "Iter: 151000; D_loss: 0.4935; G_loss: 7.023\n",
      "Iter: 152000; D_loss: 0.5778; G_loss: 6.647\n",
      "Iter: 153000; D_loss: 0.87; G_loss: 6.4\n",
      "Iter: 154000; D_loss: 0.6396; G_loss: 6.158\n",
      "Iter: 155000; D_loss: 0.4457; G_loss: 7.949\n",
      "Iter: 156000; D_loss: 0.5553; G_loss: 7.087\n",
      "Iter: 157000; D_loss: 0.7331; G_loss: 7.475\n",
      "Iter: 158000; D_loss: 0.6666; G_loss: 6.237\n",
      "Iter: 159000; D_loss: 0.5824; G_loss: 8.074\n",
      "Iter: 160000; D_loss: 0.5198; G_loss: 8.025\n",
      "Iter: 161000; D_loss: 0.4373; G_loss: 8.635\n",
      "Iter: 162000; D_loss: 0.884; G_loss: 6.115\n",
      "Iter: 163000; D_loss: 0.5962; G_loss: 7.33\n",
      "Iter: 164000; D_loss: 0.8049; G_loss: 6.397\n",
      "Iter: 165000; D_loss: 0.6714; G_loss: 7.12\n",
      "Iter: 166000; D_loss: 0.7926; G_loss: 6.182\n",
      "Iter: 167000; D_loss: 0.7021; G_loss: 6.982\n",
      "Iter: 168000; D_loss: 0.5908; G_loss: 8.417\n",
      "Iter: 169000; D_loss: 0.4816; G_loss: 7.205\n",
      "Iter: 170000; D_loss: 0.4749; G_loss: 6.827\n",
      "Iter: 171000; D_loss: 0.4972; G_loss: 8.311\n",
      "Iter: 172000; D_loss: 0.7617; G_loss: 8.888\n",
      "Iter: 173000; D_loss: 0.6254; G_loss: 8.301\n",
      "Iter: 174000; D_loss: 0.5929; G_loss: 7.014\n",
      "Iter: 175000; D_loss: 0.7748; G_loss: 7.254\n",
      "Iter: 176000; D_loss: 0.36; G_loss: 7.755\n",
      "Iter: 177000; D_loss: 0.7195; G_loss: 7.859\n",
      "Iter: 178000; D_loss: 0.6656; G_loss: 8.201\n",
      "Iter: 179000; D_loss: 0.792; G_loss: 7.309\n",
      "Iter: 180000; D_loss: 0.6459; G_loss: 6.592\n",
      "Iter: 181000; D_loss: 0.794; G_loss: 5.982\n",
      "Iter: 182000; D_loss: 0.5; G_loss: 8.651\n",
      "Iter: 183000; D_loss: 0.5119; G_loss: 9.607\n",
      "Iter: 184000; D_loss: 0.7943; G_loss: 7.261\n",
      "Iter: 185000; D_loss: 0.3747; G_loss: 7.964\n",
      "Iter: 186000; D_loss: 0.643; G_loss: 7.188\n",
      "Iter: 187000; D_loss: 0.4063; G_loss: 7.358\n",
      "Iter: 188000; D_loss: 0.5259; G_loss: 8.932\n",
      "Iter: 189000; D_loss: 0.502; G_loss: 7.968\n",
      "Iter: 190000; D_loss: 0.3955; G_loss: 7.518\n",
      "Iter: 191000; D_loss: 0.6452; G_loss: 5.925\n",
      "Iter: 192000; D_loss: 0.647; G_loss: 8.051\n",
      "Iter: 193000; D_loss: 0.6587; G_loss: 6.733\n",
      "Iter: 194000; D_loss: 0.6771; G_loss: 8.213\n",
      "Iter: 195000; D_loss: 0.872; G_loss: 6.713\n",
      "Iter: 196000; D_loss: 0.3991; G_loss: 10.35\n",
      "Iter: 197000; D_loss: 0.6764; G_loss: 7.645\n",
      "Iter: 198000; D_loss: 0.3834; G_loss: 8.847\n",
      "Iter: 199000; D_loss: 0.5509; G_loss: 8.659\n",
      "Iter: 200000; D_loss: 0.3754; G_loss: 8.585\n",
      "Iter: 201000; D_loss: 0.5614; G_loss: 7.003\n",
      "Iter: 202000; D_loss: 0.4257; G_loss: 8.08\n",
      "Iter: 203000; D_loss: 0.5339; G_loss: 8.659\n",
      "Iter: 204000; D_loss: 0.515; G_loss: 7.778\n",
      "Iter: 205000; D_loss: 0.4738; G_loss: 7.779\n",
      "Iter: 206000; D_loss: 0.5191; G_loss: 7.73\n",
      "Iter: 207000; D_loss: 0.467; G_loss: 7.634\n",
      "Iter: 208000; D_loss: 0.4734; G_loss: 10.29\n",
      "Iter: 209000; D_loss: 0.5362; G_loss: 8.315\n",
      "Iter: 210000; D_loss: 0.647; G_loss: 7.209\n",
      "Iter: 211000; D_loss: 0.6927; G_loss: 7.855\n",
      "Iter: 212000; D_loss: 0.3677; G_loss: 7.659\n",
      "Iter: 213000; D_loss: 0.4859; G_loss: 8.267\n",
      "Iter: 214000; D_loss: 0.6456; G_loss: 7.87\n",
      "Iter: 215000; D_loss: 0.7419; G_loss: 8.64\n",
      "Iter: 216000; D_loss: 0.8383; G_loss: 7.542\n",
      "Iter: 217000; D_loss: 0.7513; G_loss: 7.165\n",
      "Iter: 218000; D_loss: 0.6022; G_loss: 9.083\n",
      "Iter: 219000; D_loss: 0.57; G_loss: 8.35\n",
      "Iter: 220000; D_loss: 0.756; G_loss: 8.151\n",
      "Iter: 221000; D_loss: 0.5763; G_loss: 8.873\n",
      "Iter: 222000; D_loss: 0.5436; G_loss: 7.712\n",
      "Iter: 223000; D_loss: 0.5814; G_loss: 8.731\n",
      "Iter: 224000; D_loss: 0.5052; G_loss: 8.43\n",
      "Iter: 225000; D_loss: 0.6821; G_loss: 7.841\n",
      "Iter: 226000; D_loss: 0.5002; G_loss: 8.251\n",
      "Iter: 227000; D_loss: 0.4319; G_loss: 8.52\n",
      "Iter: 228000; D_loss: 0.6443; G_loss: 7.871\n",
      "Iter: 229000; D_loss: 0.7406; G_loss: 6.667\n",
      "Iter: 230000; D_loss: 0.6974; G_loss: 7.397\n",
      "Iter: 231000; D_loss: 0.6092; G_loss: 8.48\n",
      "Iter: 232000; D_loss: 0.4718; G_loss: 7.756\n",
      "Iter: 233000; D_loss: 0.7806; G_loss: 7.585\n",
      "Iter: 234000; D_loss: 0.4244; G_loss: 7.315\n",
      "Iter: 235000; D_loss: 1.207; G_loss: 7.216\n",
      "Iter: 236000; D_loss: 0.3784; G_loss: 9.722\n",
      "Iter: 237000; D_loss: 0.7728; G_loss: 7.294\n",
      "Iter: 238000; D_loss: 0.4761; G_loss: 7.586\n",
      "Iter: 239000; D_loss: 0.4665; G_loss: 8.738\n",
      "Iter: 240000; D_loss: 0.5143; G_loss: 9.369\n",
      "Iter: 241000; D_loss: 0.5685; G_loss: 7.518\n",
      "Iter: 242000; D_loss: 0.6917; G_loss: 7.979\n",
      "Iter: 243000; D_loss: 0.4292; G_loss: 9.447\n",
      "Iter: 244000; D_loss: 0.8697; G_loss: 7.698\n",
      "Iter: 245000; D_loss: 0.402; G_loss: 7.754\n",
      "Iter: 246000; D_loss: 0.7084; G_loss: 7.45\n",
      "Iter: 247000; D_loss: 0.5307; G_loss: 8.076\n",
      "Iter: 248000; D_loss: 0.3537; G_loss: 7.807\n",
      "Iter: 249000; D_loss: 0.3881; G_loss: 10.67\n",
      "Iter: 250000; D_loss: 0.4796; G_loss: 6.533\n",
      "Iter: 251000; D_loss: 0.6609; G_loss: 8.624\n",
      "Iter: 252000; D_loss: 0.3533; G_loss: 10.37\n",
      "Iter: 253000; D_loss: 0.4535; G_loss: 8.128\n",
      "Iter: 254000; D_loss: 0.38; G_loss: 9.857\n",
      "Iter: 255000; D_loss: 0.5231; G_loss: 9.192\n",
      "Iter: 256000; D_loss: 0.397; G_loss: 8.553\n",
      "Iter: 257000; D_loss: 0.324; G_loss: 9.789\n",
      "Iter: 258000; D_loss: 0.5368; G_loss: 10.25\n",
      "Iter: 259000; D_loss: 0.7962; G_loss: 8.438\n",
      "Iter: 260000; D_loss: 0.5848; G_loss: 7.379\n",
      "Iter: 261000; D_loss: 0.4444; G_loss: 8.868\n",
      "Iter: 262000; D_loss: 0.6287; G_loss: 9.14\n",
      "Iter: 263000; D_loss: 0.5115; G_loss: 10.69\n",
      "Iter: 264000; D_loss: 0.2479; G_loss: 11.0\n",
      "Iter: 265000; D_loss: 0.3848; G_loss: 9.597\n",
      "Iter: 266000; D_loss: 0.6007; G_loss: 8.723\n",
      "Iter: 267000; D_loss: 0.4459; G_loss: 9.236\n",
      "Iter: 268000; D_loss: 0.371; G_loss: 9.098\n",
      "Iter: 269000; D_loss: 0.4759; G_loss: 6.876\n",
      "Iter: 270000; D_loss: 0.5064; G_loss: 7.167\n",
      "Iter: 271000; D_loss: 0.3501; G_loss: 8.252\n",
      "Iter: 272000; D_loss: 0.5689; G_loss: 9.327\n",
      "Iter: 273000; D_loss: 0.8635; G_loss: 9.868\n",
      "Iter: 274000; D_loss: 0.6105; G_loss: 6.869\n",
      "Iter: 275000; D_loss: 0.8403; G_loss: 8.241\n",
      "Iter: 276000; D_loss: 0.6344; G_loss: 7.207\n",
      "Iter: 277000; D_loss: 0.4477; G_loss: 8.689\n",
      "Iter: 278000; D_loss: 0.4379; G_loss: 9.113\n",
      "Iter: 279000; D_loss: 0.6375; G_loss: 7.577\n",
      "Iter: 280000; D_loss: 0.7573; G_loss: 7.551\n",
      "Iter: 281000; D_loss: 0.3996; G_loss: 8.533\n",
      "Iter: 282000; D_loss: 0.5672; G_loss: 8.287\n",
      "Iter: 283000; D_loss: 0.3125; G_loss: 10.22\n",
      "Iter: 284000; D_loss: 0.411; G_loss: 9.633\n",
      "Iter: 285000; D_loss: 0.4218; G_loss: 8.801\n",
      "Iter: 286000; D_loss: 0.3349; G_loss: 9.875\n",
      "Iter: 287000; D_loss: 0.6078; G_loss: 8.445\n",
      "Iter: 288000; D_loss: 0.6533; G_loss: 9.17\n",
      "Iter: 289000; D_loss: 0.6878; G_loss: 6.136\n",
      "Iter: 290000; D_loss: 0.4717; G_loss: 10.15\n",
      "Iter: 291000; D_loss: 0.6557; G_loss: 9.35\n",
      "Iter: 292000; D_loss: 0.3913; G_loss: 8.631\n",
      "Iter: 293000; D_loss: 0.4197; G_loss: 10.54\n",
      "Iter: 294000; D_loss: 0.4372; G_loss: 11.21\n",
      "Iter: 295000; D_loss: 0.434; G_loss: 9.182\n",
      "Iter: 296000; D_loss: 0.3101; G_loss: 10.22\n",
      "Iter: 297000; D_loss: 0.434; G_loss: 9.314\n",
      "Iter: 298000; D_loss: 0.5439; G_loss: 8.372\n",
      "Iter: 299000; D_loss: 0.5937; G_loss: 8.291\n",
      "Iter: 300000; D_loss: 0.3294; G_loss: 11.18\n",
      "Iter: 301000; D_loss: 0.5782; G_loss: 10.54\n",
      "Iter: 302000; D_loss: 0.4651; G_loss: 9.485\n",
      "Iter: 303000; D_loss: 0.7571; G_loss: 8.879\n",
      "Iter: 304000; D_loss: 0.77; G_loss: 9.38\n",
      "Iter: 305000; D_loss: 0.5923; G_loss: 8.042\n",
      "Iter: 306000; D_loss: 0.7123; G_loss: 11.14\n",
      "Iter: 307000; D_loss: 0.4443; G_loss: 9.058\n",
      "Iter: 308000; D_loss: 0.3417; G_loss: 8.712\n",
      "Iter: 309000; D_loss: 0.3669; G_loss: 9.327\n",
      "Iter: 310000; D_loss: 0.7476; G_loss: 7.973\n",
      "Iter: 311000; D_loss: 0.4728; G_loss: 9.201\n",
      "Iter: 312000; D_loss: 0.4978; G_loss: 11.25\n",
      "Iter: 313000; D_loss: 0.5631; G_loss: 10.72\n",
      "Iter: 314000; D_loss: 0.424; G_loss: 11.4\n",
      "Iter: 315000; D_loss: 0.4798; G_loss: 9.075\n",
      "Iter: 316000; D_loss: 0.4178; G_loss: 9.662\n",
      "Iter: 317000; D_loss: 0.4661; G_loss: 9.584\n",
      "Iter: 318000; D_loss: 0.7743; G_loss: 8.467\n",
      "Iter: 319000; D_loss: 0.464; G_loss: 9.349\n",
      "Iter: 320000; D_loss: 0.4191; G_loss: 8.27\n",
      "Iter: 321000; D_loss: 0.8111; G_loss: 9.065\n",
      "Iter: 322000; D_loss: 0.4644; G_loss: 9.412\n",
      "Iter: 323000; D_loss: 0.434; G_loss: 9.755\n",
      "Iter: 324000; D_loss: 0.3528; G_loss: 9.598\n",
      "Iter: 325000; D_loss: 0.6098; G_loss: 9.629\n",
      "Iter: 326000; D_loss: 0.4154; G_loss: 10.65\n",
      "Iter: 327000; D_loss: 0.3965; G_loss: 9.378\n",
      "Iter: 328000; D_loss: 0.5279; G_loss: 10.93\n",
      "Iter: 329000; D_loss: 0.3862; G_loss: 11.83\n",
      "Iter: 330000; D_loss: 0.5022; G_loss: 10.82\n",
      "Iter: 331000; D_loss: 0.4656; G_loss: 11.35\n",
      "Iter: 332000; D_loss: 0.5547; G_loss: 8.422\n",
      "Iter: 333000; D_loss: 0.504; G_loss: 8.921\n",
      "Iter: 334000; D_loss: 0.7851; G_loss: 9.097\n",
      "Iter: 335000; D_loss: 0.45; G_loss: 8.085\n",
      "Iter: 336000; D_loss: 0.5851; G_loss: 9.884\n",
      "Iter: 337000; D_loss: 0.6276; G_loss: 12.18\n",
      "Iter: 338000; D_loss: 0.4872; G_loss: 10.14\n",
      "Iter: 339000; D_loss: 0.6394; G_loss: 9.368\n",
      "Iter: 340000; D_loss: 0.4065; G_loss: 9.495\n",
      "Iter: 341000; D_loss: 0.4735; G_loss: 8.879\n",
      "Iter: 342000; D_loss: 0.4864; G_loss: 10.49\n",
      "Iter: 343000; D_loss: 0.482; G_loss: 7.069\n",
      "Iter: 344000; D_loss: 0.5733; G_loss: 8.657\n",
      "Iter: 345000; D_loss: 0.6709; G_loss: 9.87\n",
      "Iter: 346000; D_loss: 0.528; G_loss: 9.052\n",
      "Iter: 347000; D_loss: 0.3865; G_loss: 9.377\n",
      "Iter: 348000; D_loss: 0.6491; G_loss: 11.39\n",
      "Iter: 349000; D_loss: 0.6176; G_loss: 11.07\n",
      "Iter: 350000; D_loss: 0.4608; G_loss: 10.98\n",
      "Iter: 351000; D_loss: 0.6435; G_loss: 10.93\n",
      "Iter: 352000; D_loss: 0.4657; G_loss: 7.95\n",
      "Iter: 353000; D_loss: 0.2685; G_loss: 12.36\n",
      "Iter: 354000; D_loss: 0.5064; G_loss: 12.15\n",
      "Iter: 355000; D_loss: 0.4232; G_loss: 11.22\n",
      "Iter: 356000; D_loss: 0.3135; G_loss: 9.339\n",
      "Iter: 357000; D_loss: 0.5836; G_loss: 10.15\n",
      "Iter: 358000; D_loss: 0.2984; G_loss: 10.94\n",
      "Iter: 359000; D_loss: 0.5659; G_loss: 12.24\n",
      "Iter: 360000; D_loss: 0.4901; G_loss: 8.397\n",
      "Iter: 361000; D_loss: 0.5323; G_loss: 10.95\n",
      "Iter: 362000; D_loss: 0.2911; G_loss: 11.48\n",
      "Iter: 363000; D_loss: 0.6533; G_loss: 9.509\n",
      "Iter: 364000; D_loss: 0.3721; G_loss: 10.21\n",
      "Iter: 365000; D_loss: 0.6825; G_loss: 9.656\n",
      "Iter: 366000; D_loss: 0.4366; G_loss: 11.57\n",
      "Iter: 367000; D_loss: 0.5666; G_loss: 9.069\n",
      "Iter: 368000; D_loss: 0.6524; G_loss: 8.477\n",
      "Iter: 369000; D_loss: 0.5979; G_loss: 10.68\n",
      "Iter: 370000; D_loss: 0.3045; G_loss: 12.89\n",
      "Iter: 371000; D_loss: 0.4143; G_loss: 14.05\n",
      "Iter: 372000; D_loss: 0.562; G_loss: 10.19\n",
      "Iter: 373000; D_loss: 0.396; G_loss: 11.63\n",
      "Iter: 374000; D_loss: 0.36; G_loss: 11.19\n",
      "Iter: 375000; D_loss: 0.5442; G_loss: 11.73\n",
      "Iter: 376000; D_loss: 0.5835; G_loss: 10.52\n",
      "Iter: 377000; D_loss: 0.2906; G_loss: 12.5\n",
      "Iter: 378000; D_loss: 0.4855; G_loss: 10.3\n",
      "Iter: 379000; D_loss: 0.7407; G_loss: 10.66\n",
      "Iter: 380000; D_loss: 0.4386; G_loss: 11.16\n",
      "Iter: 381000; D_loss: 0.4839; G_loss: 9.164\n",
      "Iter: 382000; D_loss: 0.3982; G_loss: 10.57\n",
      "Iter: 383000; D_loss: 0.3355; G_loss: 12.66\n",
      "Iter: 384000; D_loss: 0.5472; G_loss: 11.7\n",
      "Iter: 385000; D_loss: 0.3318; G_loss: 11.33\n",
      "Iter: 386000; D_loss: 0.631; G_loss: 10.43\n",
      "Iter: 387000; D_loss: 0.5362; G_loss: 10.53\n",
      "Iter: 388000; D_loss: 0.665; G_loss: 10.14\n",
      "Iter: 389000; D_loss: 0.6348; G_loss: 10.95\n",
      "Iter: 390000; D_loss: 0.3075; G_loss: 12.45\n",
      "Iter: 391000; D_loss: 0.4232; G_loss: 11.66\n",
      "Iter: 392000; D_loss: 0.7833; G_loss: 10.75\n",
      "Iter: 393000; D_loss: 0.4814; G_loss: 11.05\n",
      "Iter: 394000; D_loss: 0.5627; G_loss: 10.17\n",
      "Iter: 395000; D_loss: 0.6145; G_loss: 9.424\n",
      "Iter: 396000; D_loss: 0.53; G_loss: 9.016\n",
      "Iter: 397000; D_loss: 0.4998; G_loss: 11.23\n",
      "Iter: 398000; D_loss: 0.615; G_loss: 12.52\n",
      "Iter: 399000; D_loss: 0.2666; G_loss: 12.76\n",
      "Iter: 400000; D_loss: 0.3384; G_loss: 11.51\n",
      "Iter: 401000; D_loss: 0.3157; G_loss: 13.62\n",
      "Iter: 402000; D_loss: 0.3892; G_loss: 12.47\n",
      "Iter: 403000; D_loss: 0.5287; G_loss: 11.15\n",
      "Iter: 404000; D_loss: 0.6302; G_loss: 10.49\n",
      "Iter: 405000; D_loss: 0.391; G_loss: 12.4\n",
      "Iter: 406000; D_loss: 0.6505; G_loss: 11.27\n",
      "Iter: 407000; D_loss: 0.5817; G_loss: 11.64\n",
      "Iter: 408000; D_loss: 0.6215; G_loss: 11.16\n",
      "Iter: 409000; D_loss: 0.4265; G_loss: 14.13\n",
      "Iter: 410000; D_loss: 0.453; G_loss: 12.46\n",
      "Iter: 411000; D_loss: 0.3769; G_loss: 11.83\n",
      "Iter: 412000; D_loss: 0.6696; G_loss: 10.59\n",
      "Iter: 413000; D_loss: 0.5352; G_loss: 12.49\n",
      "Iter: 414000; D_loss: 0.6787; G_loss: 10.54\n",
      "Iter: 415000; D_loss: 0.5065; G_loss: 10.99\n",
      "Iter: 416000; D_loss: 0.7579; G_loss: 10.2\n",
      "Iter: 417000; D_loss: 0.6253; G_loss: 12.01\n",
      "Iter: 418000; D_loss: 0.4674; G_loss: 13.69\n",
      "Iter: 419000; D_loss: 0.7599; G_loss: 10.15\n",
      "Iter: 420000; D_loss: 0.3485; G_loss: 10.71\n",
      "Iter: 421000; D_loss: 0.8104; G_loss: 12.94\n",
      "Iter: 422000; D_loss: 0.6635; G_loss: 11.77\n",
      "Iter: 423000; D_loss: 0.3756; G_loss: 12.6\n",
      "Iter: 424000; D_loss: 0.4309; G_loss: 12.08\n",
      "Iter: 425000; D_loss: 0.2661; G_loss: 12.84\n",
      "Iter: 426000; D_loss: 0.443; G_loss: 10.35\n",
      "Iter: 427000; D_loss: 0.4058; G_loss: 11.05\n",
      "Iter: 428000; D_loss: 0.5345; G_loss: 12.58\n",
      "Iter: 429000; D_loss: 0.7625; G_loss: 9.499\n",
      "Iter: 430000; D_loss: 0.4713; G_loss: 12.37\n",
      "Iter: 431000; D_loss: 0.4086; G_loss: 13.1\n",
      "Iter: 432000; D_loss: 0.4064; G_loss: 11.66\n",
      "Iter: 433000; D_loss: 0.3571; G_loss: 12.22\n",
      "Iter: 434000; D_loss: 0.5202; G_loss: 12.33\n",
      "Iter: 435000; D_loss: 0.3699; G_loss: 12.63\n",
      "Iter: 436000; D_loss: 0.6681; G_loss: 11.2\n",
      "Iter: 437000; D_loss: 0.5789; G_loss: 12.3\n",
      "Iter: 438000; D_loss: 0.4046; G_loss: 11.86\n",
      "Iter: 439000; D_loss: 0.523; G_loss: 13.07\n",
      "Iter: 440000; D_loss: 0.6299; G_loss: 14.03\n",
      "Iter: 441000; D_loss: 0.7098; G_loss: 12.52\n",
      "Iter: 442000; D_loss: 0.4038; G_loss: 11.56\n",
      "Iter: 443000; D_loss: 0.5968; G_loss: 11.14\n",
      "Iter: 444000; D_loss: 0.5831; G_loss: 11.86\n",
      "Iter: 445000; D_loss: 0.2339; G_loss: 12.39\n",
      "Iter: 446000; D_loss: 0.4728; G_loss: 9.951\n",
      "Iter: 447000; D_loss: 0.5104; G_loss: 12.04\n",
      "Iter: 448000; D_loss: 0.415; G_loss: 12.93\n",
      "Iter: 449000; D_loss: 0.4972; G_loss: 12.93\n",
      "Iter: 450000; D_loss: 0.4868; G_loss: 11.39\n",
      "Iter: 451000; D_loss: 0.6752; G_loss: 12.14\n",
      "Iter: 452000; D_loss: 0.3875; G_loss: 13.18\n",
      "Iter: 453000; D_loss: 0.3637; G_loss: 11.85\n",
      "Iter: 454000; D_loss: 0.8053; G_loss: 11.27\n",
      "Iter: 455000; D_loss: 0.6319; G_loss: 10.81\n",
      "Iter: 456000; D_loss: 0.3861; G_loss: 13.63\n",
      "Iter: 457000; D_loss: 0.8418; G_loss: 12.84\n",
      "Iter: 458000; D_loss: 0.3507; G_loss: 11.89\n",
      "Iter: 459000; D_loss: 0.7537; G_loss: 9.619\n",
      "Iter: 460000; D_loss: 0.3299; G_loss: 12.46\n",
      "Iter: 461000; D_loss: 0.6316; G_loss: 12.07\n",
      "Iter: 462000; D_loss: 0.2562; G_loss: 16.45\n",
      "Iter: 463000; D_loss: 0.4493; G_loss: 13.7\n",
      "Iter: 464000; D_loss: 0.5232; G_loss: 10.86\n",
      "Iter: 465000; D_loss: 0.3738; G_loss: 12.51\n",
      "Iter: 466000; D_loss: 0.46; G_loss: 10.89\n",
      "Iter: 467000; D_loss: 0.511; G_loss: 12.56\n",
      "Iter: 468000; D_loss: 0.1934; G_loss: 12.88\n",
      "Iter: 469000; D_loss: 0.3775; G_loss: 12.35\n",
      "Iter: 470000; D_loss: 0.3527; G_loss: 14.21\n",
      "Iter: 471000; D_loss: 0.3138; G_loss: 15.09\n",
      "Iter: 472000; D_loss: 0.6864; G_loss: 12.67\n",
      "Iter: 473000; D_loss: 0.4712; G_loss: 12.58\n",
      "Iter: 474000; D_loss: 0.3393; G_loss: 13.22\n",
      "Iter: 475000; D_loss: 0.537; G_loss: 13.04\n",
      "Iter: 476000; D_loss: 0.3754; G_loss: 13.99\n",
      "Iter: 477000; D_loss: 0.4448; G_loss: 13.64\n",
      "Iter: 478000; D_loss: 0.8671; G_loss: 11.96\n",
      "Iter: 479000; D_loss: 0.2511; G_loss: 13.66\n",
      "Iter: 480000; D_loss: 0.2757; G_loss: 13.02\n",
      "Iter: 481000; D_loss: 0.5505; G_loss: 11.46\n",
      "Iter: 482000; D_loss: 0.4744; G_loss: 12.63\n",
      "Iter: 483000; D_loss: 0.5047; G_loss: 10.79\n",
      "Iter: 484000; D_loss: 0.3664; G_loss: 13.41\n",
      "Iter: 485000; D_loss: 0.7215; G_loss: 11.08\n",
      "Iter: 486000; D_loss: 0.3494; G_loss: 13.67\n",
      "Iter: 487000; D_loss: 0.4897; G_loss: 12.82\n",
      "Iter: 488000; D_loss: 0.1507; G_loss: 15.41\n",
      "Iter: 489000; D_loss: 0.2985; G_loss: 14.19\n",
      "Iter: 490000; D_loss: 0.4309; G_loss: 12.18\n",
      "Iter: 491000; D_loss: 0.4383; G_loss: 13.22\n",
      "Iter: 492000; D_loss: 0.5767; G_loss: 14.08\n",
      "Iter: 493000; D_loss: 0.3362; G_loss: 13.68\n",
      "Iter: 494000; D_loss: 0.4069; G_loss: 12.73\n",
      "Iter: 495000; D_loss: 0.1812; G_loss: 13.3\n",
      "Iter: 496000; D_loss: 0.3362; G_loss: 13.33\n",
      "Iter: 497000; D_loss: 0.2839; G_loss: 13.01\n",
      "Iter: 498000; D_loss: 0.5376; G_loss: 12.54\n",
      "Iter: 499000; D_loss: 0.2992; G_loss: 12.42\n",
      "Iter: 500000; D_loss: 0.4597; G_loss: 12.1\n",
      "Iter: 501000; D_loss: 0.5514; G_loss: 11.81\n",
      "Iter: 502000; D_loss: 0.4654; G_loss: 13.49\n",
      "Iter: 503000; D_loss: 0.6142; G_loss: 13.11\n",
      "Iter: 504000; D_loss: 0.3446; G_loss: 13.76\n",
      "Iter: 505000; D_loss: 0.5481; G_loss: 13.15\n",
      "Iter: 506000; D_loss: 0.3214; G_loss: 13.31\n",
      "Iter: 507000; D_loss: 0.3458; G_loss: 12.57\n",
      "Iter: 508000; D_loss: 0.4021; G_loss: 13.32\n",
      "Iter: 509000; D_loss: 0.2743; G_loss: 14.87\n",
      "Iter: 510000; D_loss: 0.4239; G_loss: 14.0\n",
      "Iter: 511000; D_loss: 0.4678; G_loss: 11.32\n",
      "Iter: 512000; D_loss: 0.3241; G_loss: 13.92\n",
      "Iter: 513000; D_loss: 0.3131; G_loss: 13.14\n",
      "Iter: 514000; D_loss: 0.3913; G_loss: 13.37\n",
      "Iter: 515000; D_loss: 0.5528; G_loss: 11.32\n",
      "Iter: 516000; D_loss: 0.3158; G_loss: 13.78\n",
      "Iter: 517000; D_loss: 0.385; G_loss: 12.89\n",
      "Iter: 518000; D_loss: 0.3182; G_loss: 13.55\n",
      "Iter: 519000; D_loss: 0.5689; G_loss: 10.42\n",
      "Iter: 520000; D_loss: 0.3469; G_loss: 12.86\n",
      "Iter: 521000; D_loss: 0.3448; G_loss: 12.59\n",
      "Iter: 522000; D_loss: 0.575; G_loss: 10.66\n",
      "Iter: 523000; D_loss: 0.2991; G_loss: 15.27\n",
      "Iter: 524000; D_loss: 0.3022; G_loss: 15.4\n",
      "Iter: 525000; D_loss: 0.4281; G_loss: 10.34\n",
      "Iter: 526000; D_loss: 0.3149; G_loss: 12.64\n",
      "Iter: 527000; D_loss: 0.4016; G_loss: 12.84\n",
      "Iter: 528000; D_loss: 0.4094; G_loss: 12.37\n",
      "Iter: 529000; D_loss: 0.7555; G_loss: 9.156\n",
      "Iter: 530000; D_loss: 0.4322; G_loss: 12.66\n",
      "Iter: 531000; D_loss: 0.3606; G_loss: 13.78\n",
      "Iter: 532000; D_loss: 0.716; G_loss: 12.68\n",
      "Iter: 533000; D_loss: 0.2306; G_loss: 15.6\n",
      "Iter: 534000; D_loss: 0.2872; G_loss: 12.79\n",
      "Iter: 535000; D_loss: 0.448; G_loss: 14.8\n",
      "Iter: 536000; D_loss: 0.2756; G_loss: 12.04\n",
      "Iter: 537000; D_loss: 0.3756; G_loss: 12.35\n",
      "Iter: 538000; D_loss: 0.3377; G_loss: 14.17\n",
      "Iter: 539000; D_loss: 0.3857; G_loss: 13.91\n",
      "Iter: 540000; D_loss: 0.4624; G_loss: 10.89\n",
      "Iter: 541000; D_loss: 0.4479; G_loss: 14.31\n",
      "Iter: 542000; D_loss: 0.5923; G_loss: 13.04\n",
      "Iter: 543000; D_loss: 0.2931; G_loss: 14.71\n",
      "Iter: 544000; D_loss: 0.2909; G_loss: 12.38\n",
      "Iter: 545000; D_loss: 0.4704; G_loss: 12.24\n",
      "Iter: 546000; D_loss: 0.3538; G_loss: 13.52\n",
      "Iter: 547000; D_loss: 0.5484; G_loss: 14.19\n",
      "Iter: 548000; D_loss: 0.4463; G_loss: 11.07\n",
      "Iter: 549000; D_loss: 0.5709; G_loss: 11.77\n",
      "Iter: 550000; D_loss: 0.702; G_loss: 8.832\n",
      "Iter: 551000; D_loss: 0.5264; G_loss: 11.99\n",
      "Iter: 552000; D_loss: 0.4691; G_loss: 15.08\n",
      "Iter: 553000; D_loss: 0.6836; G_loss: 11.43\n",
      "Iter: 554000; D_loss: 0.5273; G_loss: 10.07\n",
      "Iter: 555000; D_loss: 0.3378; G_loss: 11.59\n",
      "Iter: 556000; D_loss: 0.4165; G_loss: 13.69\n",
      "Iter: 557000; D_loss: 0.4841; G_loss: 14.04\n",
      "Iter: 558000; D_loss: 0.4586; G_loss: 10.99\n",
      "Iter: 559000; D_loss: 0.6499; G_loss: 10.86\n",
      "Iter: 560000; D_loss: 0.6184; G_loss: 10.38\n",
      "Iter: 561000; D_loss: 0.2228; G_loss: 16.52\n",
      "Iter: 562000; D_loss: 0.4911; G_loss: 14.38\n",
      "Iter: 563000; D_loss: 0.5458; G_loss: 10.66\n",
      "Iter: 564000; D_loss: 0.4175; G_loss: 12.51\n",
      "Iter: 565000; D_loss: 0.2908; G_loss: 12.45\n",
      "Iter: 566000; D_loss: 0.3291; G_loss: 13.39\n",
      "Iter: 567000; D_loss: 0.3131; G_loss: 14.43\n",
      "Iter: 568000; D_loss: 0.4681; G_loss: 12.29\n",
      "Iter: 569000; D_loss: 0.2875; G_loss: 13.3\n",
      "Iter: 570000; D_loss: 0.491; G_loss: 11.85\n",
      "Iter: 571000; D_loss: 0.5644; G_loss: 10.7\n",
      "Iter: 572000; D_loss: 0.253; G_loss: 11.31\n",
      "Iter: 573000; D_loss: 0.7546; G_loss: 9.981\n",
      "Iter: 574000; D_loss: 0.4539; G_loss: 12.0\n",
      "Iter: 575000; D_loss: 0.4763; G_loss: 12.18\n",
      "Iter: 576000; D_loss: 0.7278; G_loss: 11.9\n",
      "Iter: 577000; D_loss: 0.3731; G_loss: 13.56\n",
      "Iter: 578000; D_loss: 0.401; G_loss: 15.3\n",
      "Iter: 579000; D_loss: 0.418; G_loss: 10.3\n",
      "Iter: 580000; D_loss: 0.614; G_loss: 12.94\n",
      "Iter: 581000; D_loss: 0.3555; G_loss: 14.26\n",
      "Iter: 582000; D_loss: 0.4434; G_loss: 11.84\n",
      "Iter: 583000; D_loss: 0.5111; G_loss: 10.05\n",
      "Iter: 584000; D_loss: 0.4491; G_loss: 14.33\n",
      "Iter: 585000; D_loss: 0.2504; G_loss: 13.41\n",
      "Iter: 586000; D_loss: 0.3069; G_loss: 13.26\n",
      "Iter: 587000; D_loss: 0.495; G_loss: 11.55\n",
      "Iter: 588000; D_loss: 0.3634; G_loss: 13.55\n",
      "Iter: 589000; D_loss: 0.4041; G_loss: 12.99\n",
      "Iter: 590000; D_loss: 0.4457; G_loss: 12.65\n",
      "Iter: 591000; D_loss: 0.4718; G_loss: 13.21\n",
      "Iter: 592000; D_loss: 0.4646; G_loss: 12.53\n",
      "Iter: 593000; D_loss: 0.4295; G_loss: 12.97\n",
      "Iter: 594000; D_loss: 0.3578; G_loss: 13.85\n",
      "Iter: 595000; D_loss: 0.279; G_loss: 14.36\n",
      "Iter: 596000; D_loss: 0.3297; G_loss: 14.69\n",
      "Iter: 597000; D_loss: 0.5523; G_loss: 10.54\n",
      "Iter: 598000; D_loss: 0.5941; G_loss: 12.14\n",
      "Iter: 599000; D_loss: 0.3462; G_loss: 11.83\n",
      "Iter: 600000; D_loss: 0.3354; G_loss: 12.97\n",
      "Iter: 601000; D_loss: 0.4011; G_loss: 15.44\n",
      "Iter: 602000; D_loss: 0.4848; G_loss: 11.06\n",
      "Iter: 603000; D_loss: 0.3841; G_loss: 10.22\n",
      "Iter: 604000; D_loss: 0.5306; G_loss: 10.53\n",
      "Iter: 605000; D_loss: 0.3717; G_loss: 12.14\n",
      "Iter: 606000; D_loss: 0.4348; G_loss: 12.46\n",
      "Iter: 607000; D_loss: 0.4024; G_loss: 13.21\n",
      "Iter: 608000; D_loss: 0.3628; G_loss: 15.25\n",
      "Iter: 609000; D_loss: 0.5306; G_loss: 12.88\n",
      "Iter: 610000; D_loss: 0.4368; G_loss: 12.44\n",
      "Iter: 611000; D_loss: 0.4542; G_loss: 10.93\n",
      "Iter: 612000; D_loss: 0.7328; G_loss: 9.79\n",
      "Iter: 613000; D_loss: 0.4162; G_loss: 12.07\n",
      "Iter: 614000; D_loss: 0.6032; G_loss: 12.79\n",
      "Iter: 615000; D_loss: 0.4147; G_loss: 13.85\n",
      "Iter: 616000; D_loss: 0.7207; G_loss: 11.94\n",
      "Iter: 617000; D_loss: 0.5335; G_loss: 9.187\n",
      "Iter: 618000; D_loss: 0.1618; G_loss: 14.91\n",
      "Iter: 619000; D_loss: 0.3742; G_loss: 12.88\n",
      "Iter: 620000; D_loss: 0.3789; G_loss: 14.13\n",
      "Iter: 621000; D_loss: 0.3504; G_loss: 12.08\n",
      "Iter: 622000; D_loss: 0.3158; G_loss: 12.74\n",
      "Iter: 623000; D_loss: 0.3331; G_loss: 13.79\n",
      "Iter: 624000; D_loss: 0.5464; G_loss: 11.68\n",
      "Iter: 625000; D_loss: 0.4649; G_loss: 12.51\n",
      "Iter: 626000; D_loss: 0.6366; G_loss: 12.21\n",
      "Iter: 627000; D_loss: 0.3568; G_loss: 12.19\n",
      "Iter: 628000; D_loss: 0.6671; G_loss: 11.72\n",
      "Iter: 629000; D_loss: 0.2421; G_loss: 13.47\n",
      "Iter: 630000; D_loss: 0.3256; G_loss: 13.06\n",
      "Iter: 631000; D_loss: 0.425; G_loss: 13.3\n",
      "Iter: 632000; D_loss: 0.3284; G_loss: 12.58\n",
      "Iter: 633000; D_loss: 0.5642; G_loss: 12.28\n",
      "Iter: 634000; D_loss: 0.4475; G_loss: 12.99\n",
      "Iter: 635000; D_loss: 0.528; G_loss: 12.34\n",
      "Iter: 636000; D_loss: 0.3554; G_loss: 13.05\n",
      "Iter: 637000; D_loss: 0.6458; G_loss: 12.39\n",
      "Iter: 638000; D_loss: 0.2243; G_loss: 13.06\n",
      "Iter: 639000; D_loss: 0.526; G_loss: 12.05\n",
      "Iter: 640000; D_loss: 0.499; G_loss: 11.35\n",
      "Iter: 641000; D_loss: 0.3524; G_loss: 11.83\n",
      "Iter: 642000; D_loss: 0.496; G_loss: 12.48\n",
      "Iter: 643000; D_loss: 0.3105; G_loss: 13.32\n",
      "Iter: 644000; D_loss: 0.3485; G_loss: 13.29\n",
      "Iter: 645000; D_loss: 0.3356; G_loss: 14.27\n",
      "Iter: 646000; D_loss: 0.4345; G_loss: 12.86\n",
      "Iter: 647000; D_loss: 0.4672; G_loss: 11.59\n",
      "Iter: 648000; D_loss: 0.3718; G_loss: 13.39\n",
      "Iter: 649000; D_loss: 0.464; G_loss: 13.38\n",
      "Iter: 650000; D_loss: 0.4515; G_loss: 11.84\n",
      "Iter: 651000; D_loss: 0.6982; G_loss: 12.72\n",
      "Iter: 652000; D_loss: 0.505; G_loss: 12.86\n",
      "Iter: 653000; D_loss: 0.3637; G_loss: 12.57\n",
      "Iter: 654000; D_loss: 0.6496; G_loss: 13.88\n",
      "Iter: 655000; D_loss: 0.4274; G_loss: 13.58\n",
      "Iter: 656000; D_loss: 0.3793; G_loss: 11.18\n",
      "Iter: 657000; D_loss: 0.6055; G_loss: 11.56\n",
      "Iter: 658000; D_loss: 0.5796; G_loss: 10.02\n",
      "Iter: 659000; D_loss: 0.3352; G_loss: 14.75\n",
      "Iter: 660000; D_loss: 0.5448; G_loss: 11.04\n",
      "Iter: 661000; D_loss: 0.5057; G_loss: 12.71\n",
      "Iter: 662000; D_loss: 0.3202; G_loss: 13.51\n",
      "Iter: 663000; D_loss: 0.5515; G_loss: 13.48\n",
      "Iter: 664000; D_loss: 0.3739; G_loss: 14.7\n",
      "Iter: 665000; D_loss: 0.4451; G_loss: 13.28\n",
      "Iter: 666000; D_loss: 0.5083; G_loss: 11.02\n",
      "Iter: 667000; D_loss: 0.3355; G_loss: 12.43\n",
      "Iter: 668000; D_loss: 0.6998; G_loss: 12.46\n",
      "Iter: 669000; D_loss: 0.4589; G_loss: 11.34\n",
      "Iter: 670000; D_loss: 0.3287; G_loss: 14.2\n",
      "Iter: 671000; D_loss: 0.6019; G_loss: 12.08\n",
      "Iter: 672000; D_loss: 0.2267; G_loss: 13.59\n",
      "Iter: 673000; D_loss: 0.3825; G_loss: 11.25\n",
      "Iter: 674000; D_loss: 0.6364; G_loss: 11.06\n",
      "Iter: 675000; D_loss: 0.2271; G_loss: 14.01\n",
      "Iter: 676000; D_loss: 0.4489; G_loss: 12.11\n",
      "Iter: 677000; D_loss: 0.262; G_loss: 13.31\n",
      "Iter: 678000; D_loss: 0.6461; G_loss: 11.44\n",
      "Iter: 679000; D_loss: 0.3722; G_loss: 13.78\n",
      "Iter: 680000; D_loss: 0.7019; G_loss: 13.91\n",
      "Iter: 681000; D_loss: 0.3114; G_loss: 14.74\n",
      "Iter: 682000; D_loss: 0.5732; G_loss: 11.65\n",
      "Iter: 683000; D_loss: 0.4816; G_loss: 13.19\n",
      "Iter: 684000; D_loss: 0.4534; G_loss: 11.98\n",
      "Iter: 685000; D_loss: 0.2802; G_loss: 14.64\n",
      "Iter: 686000; D_loss: 0.4413; G_loss: 12.41\n",
      "Iter: 687000; D_loss: 0.4324; G_loss: 15.0\n",
      "Iter: 688000; D_loss: 0.3669; G_loss: 12.18\n",
      "Iter: 689000; D_loss: 0.5682; G_loss: 14.68\n",
      "Iter: 690000; D_loss: 0.3668; G_loss: 12.25\n",
      "Iter: 691000; D_loss: 0.4394; G_loss: 14.32\n",
      "Iter: 692000; D_loss: 0.3748; G_loss: 15.05\n",
      "Iter: 693000; D_loss: 0.9196; G_loss: 11.61\n",
      "Iter: 694000; D_loss: 0.4239; G_loss: 15.59\n",
      "Iter: 695000; D_loss: 0.3488; G_loss: 14.67\n",
      "Iter: 696000; D_loss: 0.4763; G_loss: 11.25\n",
      "Iter: 697000; D_loss: 0.4261; G_loss: 15.28\n",
      "Iter: 698000; D_loss: 0.3594; G_loss: 13.14\n",
      "Iter: 699000; D_loss: 0.3989; G_loss: 12.39\n",
      "Iter: 700000; D_loss: 0.6999; G_loss: 11.89\n",
      "Iter: 701000; D_loss: 0.6701; G_loss: 12.42\n",
      "Iter: 702000; D_loss: 0.1709; G_loss: 13.62\n",
      "Iter: 703000; D_loss: 0.569; G_loss: 11.93\n",
      "Iter: 704000; D_loss: 0.4517; G_loss: 13.78\n",
      "Iter: 705000; D_loss: 0.3906; G_loss: 12.02\n",
      "Iter: 706000; D_loss: 0.731; G_loss: 14.5\n",
      "Iter: 707000; D_loss: 0.6021; G_loss: 12.8\n",
      "Iter: 708000; D_loss: 0.6241; G_loss: 12.71\n",
      "Iter: 709000; D_loss: 0.6188; G_loss: 12.05\n",
      "Iter: 710000; D_loss: 0.3115; G_loss: 13.12\n",
      "Iter: 711000; D_loss: 0.3382; G_loss: 14.19\n",
      "Iter: 712000; D_loss: 0.3599; G_loss: 13.75\n",
      "Iter: 713000; D_loss: 0.3628; G_loss: 11.6\n",
      "Iter: 714000; D_loss: 0.2688; G_loss: 15.95\n",
      "Iter: 715000; D_loss: 0.5113; G_loss: 13.6\n",
      "Iter: 716000; D_loss: 0.5778; G_loss: 13.85\n",
      "Iter: 717000; D_loss: 0.5809; G_loss: 12.72\n",
      "Iter: 718000; D_loss: 0.4069; G_loss: 12.74\n",
      "Iter: 719000; D_loss: 0.3287; G_loss: 13.29\n",
      "Iter: 720000; D_loss: 0.4445; G_loss: 13.32\n",
      "Iter: 721000; D_loss: 0.5916; G_loss: 11.32\n",
      "Iter: 722000; D_loss: 0.5168; G_loss: 12.54\n",
      "Iter: 723000; D_loss: 0.4101; G_loss: 12.62\n",
      "Iter: 724000; D_loss: 0.6378; G_loss: 13.57\n",
      "Iter: 725000; D_loss: 0.2599; G_loss: 12.18\n",
      "Iter: 726000; D_loss: 0.4533; G_loss: 11.84\n",
      "Iter: 727000; D_loss: 0.3618; G_loss: 12.48\n",
      "Iter: 728000; D_loss: 0.6441; G_loss: 12.63\n",
      "Iter: 729000; D_loss: 0.3809; G_loss: 13.65\n",
      "Iter: 730000; D_loss: 0.6045; G_loss: 12.91\n",
      "Iter: 731000; D_loss: 0.4705; G_loss: 11.18\n",
      "Iter: 732000; D_loss: 0.3564; G_loss: 14.96\n",
      "Iter: 733000; D_loss: 0.593; G_loss: 12.46\n",
      "Iter: 734000; D_loss: 0.5314; G_loss: 13.1\n",
      "Iter: 735000; D_loss: 0.6252; G_loss: 11.88\n",
      "Iter: 736000; D_loss: 0.4301; G_loss: 12.24\n",
      "Iter: 737000; D_loss: 0.2659; G_loss: 15.27\n",
      "Iter: 738000; D_loss: 0.4697; G_loss: 13.13\n",
      "Iter: 739000; D_loss: 0.4657; G_loss: 10.78\n",
      "Iter: 740000; D_loss: 0.5807; G_loss: 11.97\n",
      "Iter: 741000; D_loss: 0.7962; G_loss: 11.62\n",
      "Iter: 742000; D_loss: 0.3958; G_loss: 12.54\n",
      "Iter: 743000; D_loss: 0.3205; G_loss: 13.52\n",
      "Iter: 744000; D_loss: 0.1373; G_loss: 15.11\n",
      "Iter: 745000; D_loss: 0.4338; G_loss: 13.34\n",
      "Iter: 746000; D_loss: 0.4028; G_loss: 12.59\n",
      "Iter: 747000; D_loss: 0.5286; G_loss: 13.61\n",
      "Iter: 748000; D_loss: 0.5306; G_loss: 12.98\n",
      "Iter: 749000; D_loss: 0.5357; G_loss: 13.78\n",
      "Iter: 750000; D_loss: 0.2915; G_loss: 13.0\n",
      "Iter: 751000; D_loss: 0.2102; G_loss: 14.39\n",
      "Iter: 752000; D_loss: 0.5723; G_loss: 12.82\n",
      "Iter: 753000; D_loss: 0.4597; G_loss: 12.3\n",
      "Iter: 754000; D_loss: 0.5829; G_loss: 10.58\n",
      "Iter: 755000; D_loss: 0.4093; G_loss: 11.85\n",
      "Iter: 756000; D_loss: 0.4195; G_loss: 13.14\n",
      "Iter: 757000; D_loss: 0.2473; G_loss: 13.1\n",
      "Iter: 758000; D_loss: 0.2875; G_loss: 13.5\n",
      "Iter: 759000; D_loss: 0.7654; G_loss: 15.21\n",
      "Iter: 760000; D_loss: 0.3322; G_loss: 13.27\n",
      "Iter: 761000; D_loss: 0.4944; G_loss: 12.82\n",
      "Iter: 762000; D_loss: 0.3111; G_loss: 12.86\n",
      "Iter: 763000; D_loss: 0.7016; G_loss: 11.21\n",
      "Iter: 764000; D_loss: 0.3408; G_loss: 13.28\n",
      "Iter: 765000; D_loss: 0.3027; G_loss: 13.86\n",
      "Iter: 766000; D_loss: 0.338; G_loss: 11.61\n",
      "Iter: 767000; D_loss: 0.6711; G_loss: 11.06\n",
      "Iter: 768000; D_loss: 0.2316; G_loss: 13.43\n",
      "Iter: 769000; D_loss: 0.2782; G_loss: 14.15\n",
      "Iter: 770000; D_loss: 0.4632; G_loss: 12.83\n",
      "Iter: 771000; D_loss: 0.3788; G_loss: 11.2\n",
      "Iter: 772000; D_loss: 0.3756; G_loss: 13.99\n",
      "Iter: 773000; D_loss: 0.2984; G_loss: 12.86\n",
      "Iter: 774000; D_loss: 0.4231; G_loss: 13.38\n",
      "Iter: 775000; D_loss: 0.8071; G_loss: 9.299\n",
      "Iter: 776000; D_loss: 0.4683; G_loss: 14.3\n",
      "Iter: 777000; D_loss: 0.2417; G_loss: 12.64\n",
      "Iter: 778000; D_loss: 0.3894; G_loss: 13.92\n",
      "Iter: 779000; D_loss: 0.715; G_loss: 11.02\n",
      "Iter: 780000; D_loss: 0.4787; G_loss: 12.5\n",
      "Iter: 781000; D_loss: 0.6043; G_loss: 12.05\n",
      "Iter: 782000; D_loss: 0.3407; G_loss: 14.83\n",
      "Iter: 783000; D_loss: 0.3095; G_loss: 15.1\n",
      "Iter: 784000; D_loss: 0.3211; G_loss: 12.08\n",
      "Iter: 785000; D_loss: 0.352; G_loss: 13.0\n",
      "Iter: 786000; D_loss: 0.3188; G_loss: 12.63\n",
      "Iter: 787000; D_loss: 0.715; G_loss: 12.09\n",
      "Iter: 788000; D_loss: 0.5192; G_loss: 11.01\n",
      "Iter: 789000; D_loss: 0.5742; G_loss: 9.905\n",
      "Iter: 790000; D_loss: 0.5473; G_loss: 10.91\n",
      "Iter: 791000; D_loss: 0.2916; G_loss: 12.31\n",
      "Iter: 792000; D_loss: 0.2761; G_loss: 12.73\n",
      "Iter: 793000; D_loss: 0.5149; G_loss: 11.61\n",
      "Iter: 794000; D_loss: 0.7797; G_loss: 14.2\n",
      "Iter: 795000; D_loss: 0.3841; G_loss: 13.99\n",
      "Iter: 796000; D_loss: 0.4206; G_loss: 12.71\n",
      "Iter: 797000; D_loss: 0.4005; G_loss: 13.75\n",
      "Iter: 798000; D_loss: 0.3653; G_loss: 14.07\n",
      "Iter: 799000; D_loss: 0.4968; G_loss: 10.62\n",
      "Iter: 800000; D_loss: 0.4155; G_loss: 12.41\n",
      "Iter: 801000; D_loss: 0.4296; G_loss: 12.58\n",
      "Iter: 802000; D_loss: 0.4954; G_loss: 14.18\n",
      "Iter: 803000; D_loss: 0.5011; G_loss: 10.9\n",
      "Iter: 804000; D_loss: 0.2373; G_loss: 13.66\n",
      "Iter: 805000; D_loss: 0.166; G_loss: 12.21\n",
      "Iter: 806000; D_loss: 0.3881; G_loss: 12.06\n",
      "Iter: 807000; D_loss: 0.7429; G_loss: 13.59\n",
      "Iter: 808000; D_loss: 0.5208; G_loss: 12.15\n",
      "Iter: 809000; D_loss: 0.776; G_loss: 12.56\n",
      "Iter: 810000; D_loss: 0.6003; G_loss: 13.77\n",
      "Iter: 811000; D_loss: 0.4848; G_loss: 13.08\n",
      "Iter: 812000; D_loss: 0.2962; G_loss: 14.66\n",
      "Iter: 813000; D_loss: 0.1229; G_loss: 14.8\n",
      "Iter: 814000; D_loss: 0.4288; G_loss: 12.81\n",
      "Iter: 815000; D_loss: 0.5496; G_loss: 10.92\n",
      "Iter: 816000; D_loss: 0.3678; G_loss: 13.33\n",
      "Iter: 817000; D_loss: 0.4664; G_loss: 13.71\n",
      "Iter: 818000; D_loss: 0.2996; G_loss: 13.89\n",
      "Iter: 819000; D_loss: 0.7509; G_loss: 12.04\n",
      "Iter: 820000; D_loss: 0.4827; G_loss: 10.58\n",
      "Iter: 821000; D_loss: 0.4436; G_loss: 13.23\n",
      "Iter: 822000; D_loss: 0.8712; G_loss: 12.9\n",
      "Iter: 823000; D_loss: 0.298; G_loss: 15.82\n",
      "Iter: 824000; D_loss: 0.5746; G_loss: 11.76\n",
      "Iter: 825000; D_loss: 0.4986; G_loss: 11.46\n",
      "Iter: 826000; D_loss: 0.4845; G_loss: 14.24\n",
      "Iter: 827000; D_loss: 0.3912; G_loss: 11.43\n",
      "Iter: 828000; D_loss: 0.5432; G_loss: 13.2\n",
      "Iter: 829000; D_loss: 0.5535; G_loss: 13.25\n",
      "Iter: 830000; D_loss: 0.422; G_loss: 13.29\n",
      "Iter: 831000; D_loss: 0.79; G_loss: 11.01\n",
      "Iter: 832000; D_loss: 0.4158; G_loss: 13.3\n",
      "Iter: 833000; D_loss: 0.4801; G_loss: 14.5\n",
      "Iter: 834000; D_loss: 0.6292; G_loss: 11.54\n",
      "Iter: 835000; D_loss: 0.4188; G_loss: 13.62\n",
      "Iter: 836000; D_loss: 0.3554; G_loss: 15.13\n",
      "Iter: 837000; D_loss: 0.2768; G_loss: 14.44\n",
      "Iter: 838000; D_loss: 0.406; G_loss: 11.05\n",
      "Iter: 839000; D_loss: 0.6011; G_loss: 11.63\n",
      "Iter: 840000; D_loss: 0.4263; G_loss: 12.15\n",
      "Iter: 841000; D_loss: 0.5192; G_loss: 10.69\n",
      "Iter: 842000; D_loss: 0.7031; G_loss: 14.91\n",
      "Iter: 843000; D_loss: 0.5106; G_loss: 11.33\n",
      "Iter: 844000; D_loss: 0.444; G_loss: 12.69\n",
      "Iter: 845000; D_loss: 0.4263; G_loss: 10.81\n",
      "Iter: 846000; D_loss: 0.2977; G_loss: 12.69\n",
      "Iter: 847000; D_loss: 0.5465; G_loss: 11.06\n",
      "Iter: 848000; D_loss: 0.461; G_loss: 12.31\n",
      "Iter: 849000; D_loss: 0.4541; G_loss: 13.54\n",
      "Iter: 850000; D_loss: 0.4019; G_loss: 13.92\n",
      "Iter: 851000; D_loss: 0.4063; G_loss: 13.67\n",
      "Iter: 852000; D_loss: 0.445; G_loss: 15.0\n",
      "Iter: 853000; D_loss: 0.3806; G_loss: 12.14\n",
      "Iter: 854000; D_loss: 0.5042; G_loss: 12.28\n",
      "Iter: 855000; D_loss: 0.347; G_loss: 13.39\n",
      "Iter: 856000; D_loss: 0.5331; G_loss: 11.49\n",
      "Iter: 857000; D_loss: 0.3185; G_loss: 12.75\n",
      "Iter: 858000; D_loss: 0.3798; G_loss: 13.75\n",
      "Iter: 859000; D_loss: 0.4668; G_loss: 12.06\n",
      "Iter: 860000; D_loss: 0.477; G_loss: 12.53\n",
      "Iter: 861000; D_loss: 0.5713; G_loss: 14.1\n",
      "Iter: 862000; D_loss: 0.4069; G_loss: 14.87\n",
      "Iter: 863000; D_loss: 0.4301; G_loss: 12.32\n",
      "Iter: 864000; D_loss: 0.3587; G_loss: 14.22\n",
      "Iter: 865000; D_loss: 0.5877; G_loss: 11.35\n",
      "Iter: 866000; D_loss: 0.3719; G_loss: 11.72\n",
      "Iter: 867000; D_loss: 0.5637; G_loss: 11.42\n",
      "Iter: 868000; D_loss: 0.186; G_loss: 15.02\n",
      "Iter: 869000; D_loss: 0.3697; G_loss: 13.48\n",
      "Iter: 870000; D_loss: 0.596; G_loss: 12.09\n",
      "Iter: 871000; D_loss: 0.4999; G_loss: 12.97\n",
      "Iter: 872000; D_loss: 0.4288; G_loss: 13.92\n",
      "Iter: 873000; D_loss: 0.5984; G_loss: 11.61\n",
      "Iter: 874000; D_loss: 0.5145; G_loss: 12.92\n",
      "Iter: 875000; D_loss: 0.2179; G_loss: 13.81\n",
      "Iter: 876000; D_loss: 0.4077; G_loss: 13.56\n",
      "Iter: 877000; D_loss: 0.597; G_loss: 10.95\n",
      "Iter: 878000; D_loss: 0.127; G_loss: 13.28\n",
      "Iter: 879000; D_loss: 0.3703; G_loss: 13.8\n",
      "Iter: 880000; D_loss: 0.6524; G_loss: 13.13\n",
      "Iter: 881000; D_loss: 0.6219; G_loss: 11.72\n",
      "Iter: 882000; D_loss: 0.4362; G_loss: 15.39\n",
      "Iter: 883000; D_loss: 0.5621; G_loss: 12.16\n",
      "Iter: 884000; D_loss: 0.4023; G_loss: 14.78\n",
      "Iter: 885000; D_loss: 0.2608; G_loss: 13.53\n",
      "Iter: 886000; D_loss: 0.4755; G_loss: 12.66\n",
      "Iter: 887000; D_loss: 0.3119; G_loss: 13.77\n",
      "Iter: 888000; D_loss: 0.3099; G_loss: 13.11\n",
      "Iter: 889000; D_loss: 0.5257; G_loss: 11.58\n",
      "Iter: 890000; D_loss: 0.4481; G_loss: 12.69\n",
      "Iter: 891000; D_loss: 0.3057; G_loss: 13.85\n",
      "Iter: 892000; D_loss: 0.3245; G_loss: 14.16\n",
      "Iter: 893000; D_loss: 0.4957; G_loss: 14.03\n",
      "Iter: 894000; D_loss: 0.4258; G_loss: 14.01\n",
      "Iter: 895000; D_loss: 0.5184; G_loss: 13.14\n",
      "Iter: 896000; D_loss: 0.3398; G_loss: 12.63\n",
      "Iter: 897000; D_loss: 0.5839; G_loss: 11.41\n",
      "Iter: 898000; D_loss: 0.7566; G_loss: 10.68\n",
      "Iter: 899000; D_loss: 0.1746; G_loss: 14.01\n",
      "Iter: 900000; D_loss: 0.5041; G_loss: 13.85\n",
      "Iter: 901000; D_loss: 0.4708; G_loss: 12.52\n",
      "Iter: 902000; D_loss: 0.5308; G_loss: 12.74\n",
      "Iter: 903000; D_loss: 0.4072; G_loss: 13.85\n",
      "Iter: 904000; D_loss: 0.3296; G_loss: 12.03\n",
      "Iter: 905000; D_loss: 0.5074; G_loss: 13.59\n",
      "Iter: 906000; D_loss: 0.3704; G_loss: 12.83\n",
      "Iter: 907000; D_loss: 0.543; G_loss: 10.61\n",
      "Iter: 908000; D_loss: 0.5039; G_loss: 13.64\n",
      "Iter: 909000; D_loss: 0.2325; G_loss: 14.3\n",
      "Iter: 910000; D_loss: 0.3723; G_loss: 13.1\n",
      "Iter: 911000; D_loss: 0.4998; G_loss: 13.4\n",
      "Iter: 912000; D_loss: 0.3574; G_loss: 13.25\n",
      "Iter: 913000; D_loss: 0.541; G_loss: 11.62\n",
      "Iter: 914000; D_loss: 0.3929; G_loss: 12.87\n",
      "Iter: 915000; D_loss: 0.4655; G_loss: 13.18\n",
      "Iter: 916000; D_loss: 0.2346; G_loss: 13.75\n",
      "Iter: 917000; D_loss: 0.4608; G_loss: 11.25\n",
      "Iter: 918000; D_loss: 0.3578; G_loss: 13.82\n",
      "Iter: 919000; D_loss: 0.5527; G_loss: 13.35\n",
      "Iter: 920000; D_loss: 0.401; G_loss: 12.26\n",
      "Iter: 921000; D_loss: 0.3638; G_loss: 14.49\n",
      "Iter: 922000; D_loss: 0.4993; G_loss: 13.41\n",
      "Iter: 923000; D_loss: 0.4041; G_loss: 13.7\n",
      "Iter: 924000; D_loss: 0.3265; G_loss: 13.99\n",
      "Iter: 925000; D_loss: 0.358; G_loss: 13.33\n",
      "Iter: 926000; D_loss: 0.4622; G_loss: 13.52\n",
      "Iter: 927000; D_loss: 0.4361; G_loss: 12.34\n",
      "Iter: 928000; D_loss: 0.4182; G_loss: 14.37\n",
      "Iter: 929000; D_loss: 0.531; G_loss: 14.71\n",
      "Iter: 930000; D_loss: 0.3702; G_loss: 16.38\n",
      "Iter: 931000; D_loss: 0.5721; G_loss: 11.43\n",
      "Iter: 932000; D_loss: 0.3539; G_loss: 13.91\n",
      "Iter: 933000; D_loss: 0.6435; G_loss: 13.34\n",
      "Iter: 934000; D_loss: 0.1867; G_loss: 15.18\n",
      "Iter: 935000; D_loss: 0.514; G_loss: 15.94\n",
      "Iter: 936000; D_loss: 0.4698; G_loss: 12.12\n",
      "Iter: 937000; D_loss: 0.3491; G_loss: 13.4\n",
      "Iter: 938000; D_loss: 0.3656; G_loss: 15.87\n",
      "Iter: 939000; D_loss: 0.5347; G_loss: 14.33\n",
      "Iter: 940000; D_loss: 0.4361; G_loss: 14.83\n",
      "Iter: 941000; D_loss: 0.4551; G_loss: 13.73\n",
      "Iter: 942000; D_loss: 0.4555; G_loss: 14.18\n",
      "Iter: 943000; D_loss: 0.6421; G_loss: 12.84\n",
      "Iter: 944000; D_loss: 0.5634; G_loss: 12.4\n",
      "Iter: 945000; D_loss: 0.538; G_loss: 14.68\n",
      "Iter: 946000; D_loss: 0.2721; G_loss: 12.4\n",
      "Iter: 947000; D_loss: 0.2053; G_loss: 14.47\n",
      "Iter: 948000; D_loss: 0.1555; G_loss: 16.73\n",
      "Iter: 949000; D_loss: 0.5073; G_loss: 13.61\n",
      "Iter: 950000; D_loss: 0.3531; G_loss: 13.45\n",
      "Iter: 951000; D_loss: 0.4036; G_loss: 11.23\n",
      "Iter: 952000; D_loss: 0.3482; G_loss: 14.43\n",
      "Iter: 953000; D_loss: 0.5585; G_loss: 12.82\n",
      "Iter: 954000; D_loss: 0.2865; G_loss: 11.95\n",
      "Iter: 955000; D_loss: 0.649; G_loss: 12.49\n",
      "Iter: 956000; D_loss: 0.3156; G_loss: 15.16\n",
      "Iter: 957000; D_loss: 0.3237; G_loss: 14.28\n",
      "Iter: 958000; D_loss: 0.4209; G_loss: 14.65\n",
      "Iter: 959000; D_loss: 0.2851; G_loss: 15.36\n",
      "Iter: 960000; D_loss: 0.3945; G_loss: 14.97\n",
      "Iter: 961000; D_loss: 0.4727; G_loss: 14.63\n",
      "Iter: 962000; D_loss: 0.5067; G_loss: 13.02\n",
      "Iter: 963000; D_loss: 0.4887; G_loss: 12.52\n",
      "Iter: 964000; D_loss: 0.3765; G_loss: 12.51\n",
      "Iter: 965000; D_loss: 0.6071; G_loss: 12.44\n",
      "Iter: 966000; D_loss: 0.3739; G_loss: 12.79\n",
      "Iter: 967000; D_loss: 0.3234; G_loss: 14.04\n",
      "Iter: 968000; D_loss: 0.3656; G_loss: 12.44\n",
      "Iter: 969000; D_loss: 0.6101; G_loss: 12.34\n",
      "Iter: 970000; D_loss: 0.8312; G_loss: 12.47\n",
      "Iter: 971000; D_loss: 0.4366; G_loss: 13.47\n",
      "Iter: 972000; D_loss: 0.3447; G_loss: 12.15\n",
      "Iter: 973000; D_loss: 0.3198; G_loss: 14.19\n",
      "Iter: 974000; D_loss: 0.7236; G_loss: 11.65\n",
      "Iter: 975000; D_loss: 0.3832; G_loss: 12.4\n",
      "Iter: 976000; D_loss: 0.2897; G_loss: 14.43\n",
      "Iter: 977000; D_loss: 0.5754; G_loss: 12.52\n",
      "Iter: 978000; D_loss: 0.4598; G_loss: 13.59\n",
      "Iter: 979000; D_loss: 0.2059; G_loss: 17.32\n",
      "Iter: 980000; D_loss: 0.3703; G_loss: 13.11\n",
      "Iter: 981000; D_loss: 0.2263; G_loss: 16.81\n",
      "Iter: 982000; D_loss: 0.7322; G_loss: 13.56\n",
      "Iter: 983000; D_loss: 0.4186; G_loss: 13.77\n",
      "Iter: 984000; D_loss: 0.5094; G_loss: 16.61\n",
      "Iter: 985000; D_loss: 0.4024; G_loss: 13.72\n",
      "Iter: 986000; D_loss: 0.3724; G_loss: 14.93\n",
      "Iter: 987000; D_loss: 0.3557; G_loss: 14.55\n",
      "Iter: 988000; D_loss: 0.5569; G_loss: 12.48\n",
      "Iter: 989000; D_loss: 0.3488; G_loss: 13.58\n",
      "Iter: 990000; D_loss: 0.2969; G_loss: 12.76\n",
      "Iter: 991000; D_loss: 0.2747; G_loss: 14.04\n",
      "Iter: 992000; D_loss: 0.4204; G_loss: 13.71\n",
      "Iter: 993000; D_loss: 0.2521; G_loss: 13.82\n",
      "Iter: 994000; D_loss: 0.4013; G_loss: 12.55\n",
      "Iter: 995000; D_loss: 0.5966; G_loss: 13.78\n",
      "Iter: 996000; D_loss: 0.3374; G_loss: 14.48\n",
      "Iter: 997000; D_loss: 0.22; G_loss: 16.2\n",
      "Iter: 998000; D_loss: 0.3426; G_loss: 13.31\n",
      "Iter: 999000; D_loss: 0.2312; G_loss: 15.42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "z_hat = Q(X)#真图片\n",
    "X_hat = P(z)#输入随机噪声z, 输出生成样本\n",
    "\n",
    "D_enc = D(X, z_hat) ##\n",
    "D_gen = D(X_hat, z) ##\n",
    "\n",
    "D_loss = -tf.reduce_mean(log(D_enc) + log(1 - D_gen)) #别器损失函数和生成器损失函数\n",
    "G_loss = -tf.reduce_mean(log(D_gen) + log(1 - D_enc))\n",
    "# 定义判别器和生成器的优化方法为Adam算法，关键字var_list表明最小化损失函数所更新的权重矩阵\n",
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=theta_G))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    # next_batch抽取下一个批量的图片，该方法返回一个矩阵，即shape=[mb_size,784]，每一行是一张图片\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)#32\n",
    "    z_mb = sample_z(mb_size, z_dim) #64\n",
    "\n",
    "    _, D_loss_curr = sess.run(\n",
    "        [D_solver, D_loss], feed_dict={X: X_mb, z: z_mb}\n",
    "    )\n",
    "\n",
    "    _, G_loss_curr = sess.run(\n",
    "        [G_solver, G_loss], feed_dict={X: X_mb, z: z_mb}\n",
    "    )\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(it, D_loss_curr, G_loss_curr))\n",
    "\n",
    "        samples = sess.run(X_hat, feed_dict={z: sample_z(16, z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'\n",
    "                    .format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 14:44:57.552875: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553023: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553113: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553158: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553197: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553237: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553275: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-27 14:44:57.553284: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-27 14:44:57.553567: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-27 14:44:57.562058: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Loss: 769.4\n",
      "\n",
      "Iter: 1000\n",
      "Loss: 143.2\n",
      "\n",
      "Iter: 2000\n",
      "Loss: 129.3\n",
      "\n",
      "Iter: 3000\n",
      "Loss: 121.9\n",
      "\n",
      "Iter: 4000\n",
      "Loss: 117.4\n",
      "\n",
      "Iter: 5000\n",
      "Loss: 111.3\n",
      "\n",
      "Iter: 6000\n",
      "Loss: 112.9\n",
      "\n",
      "Iter: 7000\n",
      "Loss: 105.8\n",
      "\n",
      "Iter: 8000\n",
      "Loss: 110.7\n",
      "\n",
      "Iter: 9000\n",
      "Loss: 111.0\n",
      "\n",
      "Iter: 10000\n",
      "Loss: 112.2\n",
      "\n",
      "Iter: 11000\n",
      "Loss: 104.0\n",
      "\n",
      "Iter: 12000\n",
      "Loss: 107.5\n",
      "\n",
      "Iter: 13000\n",
      "Loss: 103.7\n",
      "\n",
      "Iter: 14000\n",
      "Loss: 105.2\n",
      "\n",
      "Iter: 15000\n",
      "Loss: 106.5\n",
      "\n",
      "Iter: 16000\n",
      "Loss: 109.6\n",
      "\n",
      "Iter: 17000\n",
      "Loss: 100.0\n",
      "\n",
      "Iter: 18000\n",
      "Loss: 107.3\n",
      "\n",
      "Iter: 19000\n",
      "Loss: 109.0\n",
      "\n",
      "Iter: 20000\n",
      "Loss: 108.6\n",
      "\n",
      "Iter: 21000\n",
      "Loss: 108.4\n",
      "\n",
      "Iter: 22000\n",
      "Loss: 98.92\n",
      "\n",
      "Iter: 23000\n",
      "Loss: 104.7\n",
      "\n",
      "Iter: 24000\n",
      "Loss: 104.6\n",
      "\n",
      "Iter: 25000\n",
      "Loss: 112.0\n",
      "\n",
      "Iter: 26000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 27000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 28000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 29000\n",
      "Loss: 99.88\n",
      "\n",
      "Iter: 30000\n",
      "Loss: 105.5\n",
      "\n",
      "Iter: 31000\n",
      "Loss: 108.4\n",
      "\n",
      "Iter: 32000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 33000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 34000\n",
      "Loss: 99.7\n",
      "\n",
      "Iter: 35000\n",
      "Loss: 92.62\n",
      "\n",
      "Iter: 36000\n",
      "Loss: 104.5\n",
      "\n",
      "Iter: 37000\n",
      "Loss: 97.6\n",
      "\n",
      "Iter: 38000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 39000\n",
      "Loss: 107.6\n",
      "\n",
      "Iter: 40000\n",
      "Loss: 105.4\n",
      "\n",
      "Iter: 41000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 42000\n",
      "Loss: 105.4\n",
      "\n",
      "Iter: 43000\n",
      "Loss: 108.8\n",
      "\n",
      "Iter: 44000\n",
      "Loss: 105.1\n",
      "\n",
      "Iter: 45000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 46000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 47000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 48000\n",
      "Loss: 106.9\n",
      "\n",
      "Iter: 49000\n",
      "Loss: 106.7\n",
      "\n",
      "Iter: 50000\n",
      "Loss: 99.69\n",
      "\n",
      "Iter: 51000\n",
      "Loss: 103.4\n",
      "\n",
      "Iter: 52000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 53000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 54000\n",
      "Loss: 106.1\n",
      "\n",
      "Iter: 55000\n",
      "Loss: 99.17\n",
      "\n",
      "Iter: 56000\n",
      "Loss: 103.3\n",
      "\n",
      "Iter: 57000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 58000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 59000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 60000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 61000\n",
      "Loss: 104.3\n",
      "\n",
      "Iter: 62000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 63000\n",
      "Loss: 104.8\n",
      "\n",
      "Iter: 64000\n",
      "Loss: 95.82\n",
      "\n",
      "Iter: 65000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 66000\n",
      "Loss: 107.8\n",
      "\n",
      "Iter: 67000\n",
      "Loss: 101.8\n",
      "\n",
      "Iter: 68000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 69000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 70000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 71000\n",
      "Loss: 102.1\n",
      "\n",
      "Iter: 72000\n",
      "Loss: 97.06\n",
      "\n",
      "Iter: 73000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 74000\n",
      "Loss: 107.6\n",
      "\n",
      "Iter: 75000\n",
      "Loss: 106.6\n",
      "\n",
      "Iter: 76000\n",
      "Loss: 95.96\n",
      "\n",
      "Iter: 77000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 78000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 79000\n",
      "Loss: 105.5\n",
      "\n",
      "Iter: 80000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 81000\n",
      "Loss: 104.7\n",
      "\n",
      "Iter: 82000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 83000\n",
      "Loss: 93.19\n",
      "\n",
      "Iter: 84000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 85000\n",
      "Loss: 104.4\n",
      "\n",
      "Iter: 86000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 87000\n",
      "Loss: 98.61\n",
      "\n",
      "Iter: 88000\n",
      "Loss: 103.4\n",
      "\n",
      "Iter: 89000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 90000\n",
      "Loss: 103.0\n",
      "\n",
      "Iter: 91000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 92000\n",
      "Loss: 104.4\n",
      "\n",
      "Iter: 93000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 94000\n",
      "Loss: 105.1\n",
      "\n",
      "Iter: 95000\n",
      "Loss: 97.93\n",
      "\n",
      "Iter: 96000\n",
      "Loss: 99.52\n",
      "\n",
      "Iter: 97000\n",
      "Loss: 104.1\n",
      "\n",
      "Iter: 98000\n",
      "Loss: 102.1\n",
      "\n",
      "Iter: 99000\n",
      "Loss: 99.45\n",
      "\n",
      "Iter: 100000\n",
      "Loss: 106.8\n",
      "\n",
      "Iter: 101000\n",
      "Loss: 106.5\n",
      "\n",
      "Iter: 102000\n",
      "Loss: 105.9\n",
      "\n",
      "Iter: 103000\n",
      "Loss: 104.0\n",
      "\n",
      "Iter: 104000\n",
      "Loss: 102.7\n",
      "\n",
      "Iter: 105000\n",
      "Loss: 99.56\n",
      "\n",
      "Iter: 106000\n",
      "Loss: 106.0\n",
      "\n",
      "Iter: 107000\n",
      "Loss: 99.86\n",
      "\n",
      "Iter: 108000\n",
      "Loss: 97.61\n",
      "\n",
      "Iter: 109000\n",
      "Loss: 99.14\n",
      "\n",
      "Iter: 110000\n",
      "Loss: 104.1\n",
      "\n",
      "Iter: 111000\n",
      "Loss: 99.76\n",
      "\n",
      "Iter: 112000\n",
      "Loss: 97.75\n",
      "\n",
      "Iter: 113000\n",
      "Loss: 102.1\n",
      "\n",
      "Iter: 114000\n",
      "Loss: 96.93\n",
      "\n",
      "Iter: 115000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 116000\n",
      "Loss: 98.71\n",
      "\n",
      "Iter: 117000\n",
      "Loss: 97.35\n",
      "\n",
      "Iter: 118000\n",
      "Loss: 105.6\n",
      "\n",
      "Iter: 119000\n",
      "Loss: 97.49\n",
      "\n",
      "Iter: 120000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 121000\n",
      "Loss: 96.91\n",
      "\n",
      "Iter: 122000\n",
      "Loss: 106.5\n",
      "\n",
      "Iter: 123000\n",
      "Loss: 99.69\n",
      "\n",
      "Iter: 124000\n",
      "Loss: 104.7\n",
      "\n",
      "Iter: 125000\n",
      "Loss: 108.0\n",
      "\n",
      "Iter: 126000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 127000\n",
      "Loss: 96.53\n",
      "\n",
      "Iter: 128000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 129000\n",
      "Loss: 104.1\n",
      "\n",
      "Iter: 130000\n",
      "Loss: 98.06\n",
      "\n",
      "Iter: 131000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 132000\n",
      "Loss: 97.32\n",
      "\n",
      "Iter: 133000\n",
      "Loss: 106.5\n",
      "\n",
      "Iter: 134000\n",
      "Loss: 105.7\n",
      "\n",
      "Iter: 135000\n",
      "Loss: 99.7\n",
      "\n",
      "Iter: 136000\n",
      "Loss: 98.52\n",
      "\n",
      "Iter: 137000\n",
      "Loss: 97.39\n",
      "\n",
      "Iter: 138000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 139000\n",
      "Loss: 98.6\n",
      "\n",
      "Iter: 140000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 141000\n",
      "Loss: 105.6\n",
      "\n",
      "Iter: 142000\n",
      "Loss: 97.52\n",
      "\n",
      "Iter: 143000\n",
      "Loss: 99.3\n",
      "\n",
      "Iter: 144000\n",
      "Loss: 105.7\n",
      "\n",
      "Iter: 145000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 146000\n",
      "Loss: 98.12\n",
      "\n",
      "Iter: 147000\n",
      "Loss: 97.41\n",
      "\n",
      "Iter: 148000\n",
      "Loss: 98.99\n",
      "\n",
      "Iter: 149000\n",
      "Loss: 102.1\n",
      "\n",
      "Iter: 150000\n",
      "Loss: 106.5\n",
      "\n",
      "Iter: 151000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 152000\n",
      "Loss: 104.4\n",
      "\n",
      "Iter: 153000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 154000\n",
      "Loss: 106.3\n",
      "\n",
      "Iter: 155000\n",
      "Loss: 99.72\n",
      "\n",
      "Iter: 156000\n",
      "Loss: 99.34\n",
      "\n",
      "Iter: 157000\n",
      "Loss: 104.3\n",
      "\n",
      "Iter: 158000\n",
      "Loss: 99.83\n",
      "\n",
      "Iter: 159000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 160000\n",
      "Loss: 99.82\n",
      "\n",
      "Iter: 161000\n",
      "Loss: 95.42\n",
      "\n",
      "Iter: 162000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 163000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 164000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 165000\n",
      "Loss: 93.34\n",
      "\n",
      "Iter: 166000\n",
      "Loss: 96.17\n",
      "\n",
      "Iter: 167000\n",
      "Loss: 99.19\n",
      "\n",
      "Iter: 168000\n",
      "Loss: 97.97\n",
      "\n",
      "Iter: 169000\n",
      "Loss: 99.15\n",
      "\n",
      "Iter: 170000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 171000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 172000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 173000\n",
      "Loss: 99.85\n",
      "\n",
      "Iter: 174000\n",
      "Loss: 94.69\n",
      "\n",
      "Iter: 175000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 176000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 177000\n",
      "Loss: 103.7\n",
      "\n",
      "Iter: 178000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 179000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 180000\n",
      "Loss: 98.05\n",
      "\n",
      "Iter: 181000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 182000\n",
      "Loss: 99.69\n",
      "\n",
      "Iter: 183000\n",
      "Loss: 106.7\n",
      "\n",
      "Iter: 184000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 185000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 186000\n",
      "Loss: 106.2\n",
      "\n",
      "Iter: 187000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 188000\n",
      "Loss: 98.89\n",
      "\n",
      "Iter: 189000\n",
      "Loss: 96.48\n",
      "\n",
      "Iter: 190000\n",
      "Loss: 99.72\n",
      "\n",
      "Iter: 191000\n",
      "Loss: 105.5\n",
      "\n",
      "Iter: 192000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 193000\n",
      "Loss: 104.3\n",
      "\n",
      "Iter: 194000\n",
      "Loss: 97.42\n",
      "\n",
      "Iter: 195000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 196000\n",
      "Loss: 99.63\n",
      "\n",
      "Iter: 197000\n",
      "Loss: 107.9\n",
      "\n",
      "Iter: 198000\n",
      "Loss: 99.12\n",
      "\n",
      "Iter: 199000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 200000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 201000\n",
      "Loss: 103.4\n",
      "\n",
      "Iter: 202000\n",
      "Loss: 103.4\n",
      "\n",
      "Iter: 203000\n",
      "Loss: 109.5\n",
      "\n",
      "Iter: 204000\n",
      "Loss: 97.04\n",
      "\n",
      "Iter: 205000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 206000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 207000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 208000\n",
      "Loss: 96.89\n",
      "\n",
      "Iter: 209000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 210000\n",
      "Loss: 98.26\n",
      "\n",
      "Iter: 211000\n",
      "Loss: 98.01\n",
      "\n",
      "Iter: 212000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 213000\n",
      "Loss: 104.8\n",
      "\n",
      "Iter: 214000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 215000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 216000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 217000\n",
      "Loss: 98.04\n",
      "\n",
      "Iter: 218000\n",
      "Loss: 104.6\n",
      "\n",
      "Iter: 219000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 220000\n",
      "Loss: 99.55\n",
      "\n",
      "Iter: 221000\n",
      "Loss: 99.25\n",
      "\n",
      "Iter: 222000\n",
      "Loss: 99.62\n",
      "\n",
      "Iter: 223000\n",
      "Loss: 104.3\n",
      "\n",
      "Iter: 224000\n",
      "Loss: 98.61\n",
      "\n",
      "Iter: 225000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 226000\n",
      "Loss: 96.29\n",
      "\n",
      "Iter: 227000\n",
      "Loss: 105.9\n",
      "\n",
      "Iter: 228000\n",
      "Loss: 96.88\n",
      "\n",
      "Iter: 229000\n",
      "Loss: 107.1\n",
      "\n",
      "Iter: 230000\n",
      "Loss: 96.88\n",
      "\n",
      "Iter: 231000\n",
      "Loss: 108.9\n",
      "\n",
      "Iter: 232000\n",
      "Loss: 96.2\n",
      "\n",
      "Iter: 233000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 234000\n",
      "Loss: 99.25\n",
      "\n",
      "Iter: 235000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 236000\n",
      "Loss: 94.82\n",
      "\n",
      "Iter: 237000\n",
      "Loss: 95.75\n",
      "\n",
      "Iter: 238000\n",
      "Loss: 99.82\n",
      "\n",
      "Iter: 239000\n",
      "Loss: 104.8\n",
      "\n",
      "Iter: 240000\n",
      "Loss: 99.25\n",
      "\n",
      "Iter: 241000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 242000\n",
      "Loss: 97.56\n",
      "\n",
      "Iter: 243000\n",
      "Loss: 104.6\n",
      "\n",
      "Iter: 244000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 245000\n",
      "Loss: 97.45\n",
      "\n",
      "Iter: 246000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 247000\n",
      "Loss: 97.01\n",
      "\n",
      "Iter: 248000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 249000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 250000\n",
      "Loss: 104.2\n",
      "\n",
      "Iter: 251000\n",
      "Loss: 99.82\n",
      "\n",
      "Iter: 252000\n",
      "Loss: 96.51\n",
      "\n",
      "Iter: 253000\n",
      "Loss: 103.7\n",
      "\n",
      "Iter: 254000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 255000\n",
      "Loss: 97.06\n",
      "\n",
      "Iter: 256000\n",
      "Loss: 97.35\n",
      "\n",
      "Iter: 257000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 258000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 259000\n",
      "Loss: 96.56\n",
      "\n",
      "Iter: 260000\n",
      "Loss: 97.14\n",
      "\n",
      "Iter: 261000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 262000\n",
      "Loss: 93.23\n",
      "\n",
      "Iter: 263000\n",
      "Loss: 102.1\n",
      "\n",
      "Iter: 264000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 265000\n",
      "Loss: 94.91\n",
      "\n",
      "Iter: 266000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 267000\n",
      "Loss: 97.52\n",
      "\n",
      "Iter: 268000\n",
      "Loss: 105.3\n",
      "\n",
      "Iter: 269000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 270000\n",
      "Loss: 99.59\n",
      "\n",
      "Iter: 271000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 272000\n",
      "Loss: 104.3\n",
      "\n",
      "Iter: 273000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 274000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 275000\n",
      "Loss: 105.5\n",
      "\n",
      "Iter: 276000\n",
      "Loss: 104.8\n",
      "\n",
      "Iter: 277000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 278000\n",
      "Loss: 104.5\n",
      "\n",
      "Iter: 279000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 280000\n",
      "Loss: 104.7\n",
      "\n",
      "Iter: 281000\n",
      "Loss: 97.35\n",
      "\n",
      "Iter: 282000\n",
      "Loss: 97.96\n",
      "\n",
      "Iter: 283000\n",
      "Loss: 96.2\n",
      "\n",
      "Iter: 284000\n",
      "Loss: 104.5\n",
      "\n",
      "Iter: 285000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 286000\n",
      "Loss: 103.6\n",
      "\n",
      "Iter: 287000\n",
      "Loss: 97.31\n",
      "\n",
      "Iter: 288000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 289000\n",
      "Loss: 99.34\n",
      "\n",
      "Iter: 290000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 291000\n",
      "Loss: 105.5\n",
      "\n",
      "Iter: 292000\n",
      "Loss: 94.05\n",
      "\n",
      "Iter: 293000\n",
      "Loss: 98.28\n",
      "\n",
      "Iter: 294000\n",
      "Loss: 95.67\n",
      "\n",
      "Iter: 295000\n",
      "Loss: 95.99\n",
      "\n",
      "Iter: 296000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 297000\n",
      "Loss: 98.1\n",
      "\n",
      "Iter: 298000\n",
      "Loss: 98.64\n",
      "\n",
      "Iter: 299000\n",
      "Loss: 104.4\n",
      "\n",
      "Iter: 300000\n",
      "Loss: 109.2\n",
      "\n",
      "Iter: 301000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 302000\n",
      "Loss: 103.0\n",
      "\n",
      "Iter: 303000\n",
      "Loss: 94.19\n",
      "\n",
      "Iter: 304000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 305000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 306000\n",
      "Loss: 99.97\n",
      "\n",
      "Iter: 307000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 308000\n",
      "Loss: 95.92\n",
      "\n",
      "Iter: 309000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 310000\n",
      "Loss: 99.5\n",
      "\n",
      "Iter: 311000\n",
      "Loss: 105.7\n",
      "\n",
      "Iter: 312000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 313000\n",
      "Loss: 106.1\n",
      "\n",
      "Iter: 314000\n",
      "Loss: 96.79\n",
      "\n",
      "Iter: 315000\n",
      "Loss: 99.96\n",
      "\n",
      "Iter: 316000\n",
      "Loss: 93.79\n",
      "\n",
      "Iter: 317000\n",
      "Loss: 90.8\n",
      "\n",
      "Iter: 318000\n",
      "Loss: 108.5\n",
      "\n",
      "Iter: 319000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 320000\n",
      "Loss: 103.6\n",
      "\n",
      "Iter: 321000\n",
      "Loss: 98.42\n",
      "\n",
      "Iter: 322000\n",
      "Loss: 99.85\n",
      "\n",
      "Iter: 323000\n",
      "Loss: 98.16\n",
      "\n",
      "Iter: 324000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 325000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 326000\n",
      "Loss: 104.2\n",
      "\n",
      "Iter: 327000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 328000\n",
      "Loss: 98.33\n",
      "\n",
      "Iter: 329000\n",
      "Loss: 93.77\n",
      "\n",
      "Iter: 330000\n",
      "Loss: 98.86\n",
      "\n",
      "Iter: 331000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 332000\n",
      "Loss: 98.1\n",
      "\n",
      "Iter: 333000\n",
      "Loss: 98.57\n",
      "\n",
      "Iter: 334000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 335000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 336000\n",
      "Loss: 99.12\n",
      "\n",
      "Iter: 337000\n",
      "Loss: 98.47\n",
      "\n",
      "Iter: 338000\n",
      "Loss: 104.6\n",
      "\n",
      "Iter: 339000\n",
      "Loss: 99.75\n",
      "\n",
      "Iter: 340000\n",
      "Loss: 97.9\n",
      "\n",
      "Iter: 341000\n",
      "Loss: 104.6\n",
      "\n",
      "Iter: 342000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 343000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 344000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 345000\n",
      "Loss: 96.11\n",
      "\n",
      "Iter: 346000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 347000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 348000\n",
      "Loss: 99.34\n",
      "\n",
      "Iter: 349000\n",
      "Loss: 97.32\n",
      "\n",
      "Iter: 350000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 351000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 352000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 353000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 354000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 355000\n",
      "Loss: 107.0\n",
      "\n",
      "Iter: 356000\n",
      "Loss: 97.22\n",
      "\n",
      "Iter: 357000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 358000\n",
      "Loss: 99.61\n",
      "\n",
      "Iter: 359000\n",
      "Loss: 95.03\n",
      "\n",
      "Iter: 360000\n",
      "Loss: 95.0\n",
      "\n",
      "Iter: 361000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 362000\n",
      "Loss: 97.18\n",
      "\n",
      "Iter: 363000\n",
      "Loss: 95.7\n",
      "\n",
      "Iter: 364000\n",
      "Loss: 99.1\n",
      "\n",
      "Iter: 365000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 366000\n",
      "Loss: 99.79\n",
      "\n",
      "Iter: 367000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 368000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 369000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 370000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 371000\n",
      "Loss: 99.17\n",
      "\n",
      "Iter: 372000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 373000\n",
      "Loss: 97.22\n",
      "\n",
      "Iter: 374000\n",
      "Loss: 97.51\n",
      "\n",
      "Iter: 375000\n",
      "Loss: 99.94\n",
      "\n",
      "Iter: 376000\n",
      "Loss: 101.8\n",
      "\n",
      "Iter: 377000\n",
      "Loss: 99.05\n",
      "\n",
      "Iter: 378000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 379000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 380000\n",
      "Loss: 96.49\n",
      "\n",
      "Iter: 381000\n",
      "Loss: 99.91\n",
      "\n",
      "Iter: 382000\n",
      "Loss: 99.96\n",
      "\n",
      "Iter: 383000\n",
      "Loss: 99.68\n",
      "\n",
      "Iter: 384000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 385000\n",
      "Loss: 102.9\n",
      "\n",
      "Iter: 386000\n",
      "Loss: 96.41\n",
      "\n",
      "Iter: 387000\n",
      "Loss: 94.98\n",
      "\n",
      "Iter: 388000\n",
      "Loss: 98.62\n",
      "\n",
      "Iter: 389000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 390000\n",
      "Loss: 98.14\n",
      "\n",
      "Iter: 391000\n",
      "Loss: 108.4\n",
      "\n",
      "Iter: 392000\n",
      "Loss: 103.3\n",
      "\n",
      "Iter: 393000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 394000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 395000\n",
      "Loss: 96.03\n",
      "\n",
      "Iter: 396000\n",
      "Loss: 94.2\n",
      "\n",
      "Iter: 397000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 398000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 399000\n",
      "Loss: 97.58\n",
      "\n",
      "Iter: 400000\n",
      "Loss: 100.0\n",
      "\n",
      "Iter: 401000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 402000\n",
      "Loss: 99.09\n",
      "\n",
      "Iter: 403000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 404000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 405000\n",
      "Loss: 103.8\n",
      "\n",
      "Iter: 406000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 407000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 408000\n",
      "Loss: 96.88\n",
      "\n",
      "Iter: 409000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 410000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 411000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 412000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 413000\n",
      "Loss: 99.39\n",
      "\n",
      "Iter: 414000\n",
      "Loss: 98.32\n",
      "\n",
      "Iter: 415000\n",
      "Loss: 95.15\n",
      "\n",
      "Iter: 416000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 417000\n",
      "Loss: 98.98\n",
      "\n",
      "Iter: 418000\n",
      "Loss: 95.96\n",
      "\n",
      "Iter: 419000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 420000\n",
      "Loss: 94.78\n",
      "\n",
      "Iter: 421000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 422000\n",
      "Loss: 99.0\n",
      "\n",
      "Iter: 423000\n",
      "Loss: 94.25\n",
      "\n",
      "Iter: 424000\n",
      "Loss: 99.39\n",
      "\n",
      "Iter: 425000\n",
      "Loss: 96.81\n",
      "\n",
      "Iter: 426000\n",
      "Loss: 97.04\n",
      "\n",
      "Iter: 427000\n",
      "Loss: 97.51\n",
      "\n",
      "Iter: 428000\n",
      "Loss: 96.53\n",
      "\n",
      "Iter: 429000\n",
      "Loss: 98.57\n",
      "\n",
      "Iter: 430000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 431000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 432000\n",
      "Loss: 101.8\n",
      "\n",
      "Iter: 433000\n",
      "Loss: 99.56\n",
      "\n",
      "Iter: 434000\n",
      "Loss: 99.64\n",
      "\n",
      "Iter: 435000\n",
      "Loss: 98.26\n",
      "\n",
      "Iter: 436000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 437000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 438000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 439000\n",
      "Loss: 96.15\n",
      "\n",
      "Iter: 440000\n",
      "Loss: 96.22\n",
      "\n",
      "Iter: 441000\n",
      "Loss: 99.86\n",
      "\n",
      "Iter: 442000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 443000\n",
      "Loss: 103.7\n",
      "\n",
      "Iter: 444000\n",
      "Loss: 95.05\n",
      "\n",
      "Iter: 445000\n",
      "Loss: 101.8\n",
      "\n",
      "Iter: 446000\n",
      "Loss: 96.25\n",
      "\n",
      "Iter: 447000\n",
      "Loss: 94.4\n",
      "\n",
      "Iter: 448000\n",
      "Loss: 94.15\n",
      "\n",
      "Iter: 449000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 450000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 451000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 452000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 453000\n",
      "Loss: 97.79\n",
      "\n",
      "Iter: 454000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 455000\n",
      "Loss: 96.63\n",
      "\n",
      "Iter: 456000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 457000\n",
      "Loss: 107.5\n",
      "\n",
      "Iter: 458000\n",
      "Loss: 98.15\n",
      "\n",
      "Iter: 459000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 460000\n",
      "Loss: 95.95\n",
      "\n",
      "Iter: 461000\n",
      "Loss: 96.85\n",
      "\n",
      "Iter: 462000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 463000\n",
      "Loss: 97.63\n",
      "\n",
      "Iter: 464000\n",
      "Loss: 102.7\n",
      "\n",
      "Iter: 465000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 466000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 467000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 468000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 469000\n",
      "Loss: 93.83\n",
      "\n",
      "Iter: 470000\n",
      "Loss: 96.48\n",
      "\n",
      "Iter: 471000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 472000\n",
      "Loss: 99.61\n",
      "\n",
      "Iter: 473000\n",
      "Loss: 94.83\n",
      "\n",
      "Iter: 474000\n",
      "Loss: 105.4\n",
      "\n",
      "Iter: 475000\n",
      "Loss: 107.4\n",
      "\n",
      "Iter: 476000\n",
      "Loss: 105.5\n",
      "\n",
      "Iter: 477000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 478000\n",
      "Loss: 99.1\n",
      "\n",
      "Iter: 479000\n",
      "Loss: 96.67\n",
      "\n",
      "Iter: 480000\n",
      "Loss: 98.38\n",
      "\n",
      "Iter: 481000\n",
      "Loss: 99.88\n",
      "\n",
      "Iter: 482000\n",
      "Loss: 99.04\n",
      "\n",
      "Iter: 483000\n",
      "Loss: 97.09\n",
      "\n",
      "Iter: 484000\n",
      "Loss: 103.8\n",
      "\n",
      "Iter: 485000\n",
      "Loss: 92.93\n",
      "\n",
      "Iter: 486000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 487000\n",
      "Loss: 104.1\n",
      "\n",
      "Iter: 488000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 489000\n",
      "Loss: 95.62\n",
      "\n",
      "Iter: 490000\n",
      "Loss: 99.43\n",
      "\n",
      "Iter: 491000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 492000\n",
      "Loss: 104.8\n",
      "\n",
      "Iter: 493000\n",
      "Loss: 99.23\n",
      "\n",
      "Iter: 494000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 495000\n",
      "Loss: 96.56\n",
      "\n",
      "Iter: 496000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 497000\n",
      "Loss: 94.97\n",
      "\n",
      "Iter: 498000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 499000\n",
      "Loss: 95.59\n",
      "\n",
      "Iter: 500000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 501000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 502000\n",
      "Loss: 98.42\n",
      "\n",
      "Iter: 503000\n",
      "Loss: 96.58\n",
      "\n",
      "Iter: 504000\n",
      "Loss: 96.15\n",
      "\n",
      "Iter: 505000\n",
      "Loss: 99.68\n",
      "\n",
      "Iter: 506000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 507000\n",
      "Loss: 97.44\n",
      "\n",
      "Iter: 508000\n",
      "Loss: 103.9\n",
      "\n",
      "Iter: 509000\n",
      "Loss: 96.71\n",
      "\n",
      "Iter: 510000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 511000\n",
      "Loss: 98.13\n",
      "\n",
      "Iter: 512000\n",
      "Loss: 96.5\n",
      "\n",
      "Iter: 513000\n",
      "Loss: 96.32\n",
      "\n",
      "Iter: 514000\n",
      "Loss: 97.92\n",
      "\n",
      "Iter: 515000\n",
      "Loss: 93.18\n",
      "\n",
      "Iter: 516000\n",
      "Loss: 99.59\n",
      "\n",
      "Iter: 517000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 518000\n",
      "Loss: 99.15\n",
      "\n",
      "Iter: 519000\n",
      "Loss: 105.1\n",
      "\n",
      "Iter: 520000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 521000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 522000\n",
      "Loss: 96.6\n",
      "\n",
      "Iter: 523000\n",
      "Loss: 93.55\n",
      "\n",
      "Iter: 524000\n",
      "Loss: 96.78\n",
      "\n",
      "Iter: 525000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 526000\n",
      "Loss: 96.59\n",
      "\n",
      "Iter: 527000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 528000\n",
      "Loss: 98.88\n",
      "\n",
      "Iter: 529000\n",
      "Loss: 103.7\n",
      "\n",
      "Iter: 530000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 531000\n",
      "Loss: 99.59\n",
      "\n",
      "Iter: 532000\n",
      "Loss: 98.66\n",
      "\n",
      "Iter: 533000\n",
      "Loss: 99.56\n",
      "\n",
      "Iter: 534000\n",
      "Loss: 105.4\n",
      "\n",
      "Iter: 535000\n",
      "Loss: 98.82\n",
      "\n",
      "Iter: 536000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 537000\n",
      "Loss: 108.3\n",
      "\n",
      "Iter: 538000\n",
      "Loss: 99.46\n",
      "\n",
      "Iter: 539000\n",
      "Loss: 96.01\n",
      "\n",
      "Iter: 540000\n",
      "Loss: 106.5\n",
      "\n",
      "Iter: 541000\n",
      "Loss: 99.06\n",
      "\n",
      "Iter: 542000\n",
      "Loss: 97.25\n",
      "\n",
      "Iter: 543000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 544000\n",
      "Loss: 95.13\n",
      "\n",
      "Iter: 545000\n",
      "Loss: 99.48\n",
      "\n",
      "Iter: 546000\n",
      "Loss: 105.2\n",
      "\n",
      "Iter: 547000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 548000\n",
      "Loss: 98.16\n",
      "\n",
      "Iter: 549000\n",
      "Loss: 97.64\n",
      "\n",
      "Iter: 550000\n",
      "Loss: 107.4\n",
      "\n",
      "Iter: 551000\n",
      "Loss: 96.2\n",
      "\n",
      "Iter: 552000\n",
      "Loss: 105.2\n",
      "\n",
      "Iter: 553000\n",
      "Loss: 96.41\n",
      "\n",
      "Iter: 554000\n",
      "Loss: 90.31\n",
      "\n",
      "Iter: 555000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 556000\n",
      "Loss: 95.79\n",
      "\n",
      "Iter: 557000\n",
      "Loss: 98.57\n",
      "\n",
      "Iter: 558000\n",
      "Loss: 94.69\n",
      "\n",
      "Iter: 559000\n",
      "Loss: 99.84\n",
      "\n",
      "Iter: 560000\n",
      "Loss: 94.09\n",
      "\n",
      "Iter: 561000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 562000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 563000\n",
      "Loss: 98.34\n",
      "\n",
      "Iter: 564000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 565000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 566000\n",
      "Loss: 98.71\n",
      "\n",
      "Iter: 567000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 568000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 569000\n",
      "Loss: 96.31\n",
      "\n",
      "Iter: 570000\n",
      "Loss: 106.2\n",
      "\n",
      "Iter: 571000\n",
      "Loss: 88.81\n",
      "\n",
      "Iter: 572000\n",
      "Loss: 99.6\n",
      "\n",
      "Iter: 573000\n",
      "Loss: 99.22\n",
      "\n",
      "Iter: 574000\n",
      "Loss: 99.98\n",
      "\n",
      "Iter: 575000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 576000\n",
      "Loss: 105.8\n",
      "\n",
      "Iter: 577000\n",
      "Loss: 99.62\n",
      "\n",
      "Iter: 578000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 579000\n",
      "Loss: 102.9\n",
      "\n",
      "Iter: 580000\n",
      "Loss: 97.67\n",
      "\n",
      "Iter: 581000\n",
      "Loss: 96.47\n",
      "\n",
      "Iter: 582000\n",
      "Loss: 97.82\n",
      "\n",
      "Iter: 583000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 584000\n",
      "Loss: 108.1\n",
      "\n",
      "Iter: 585000\n",
      "Loss: 102.1\n",
      "\n",
      "Iter: 586000\n",
      "Loss: 105.3\n",
      "\n",
      "Iter: 587000\n",
      "Loss: 98.83\n",
      "\n",
      "Iter: 588000\n",
      "Loss: 103.4\n",
      "\n",
      "Iter: 589000\n",
      "Loss: 98.76\n",
      "\n",
      "Iter: 590000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 591000\n",
      "Loss: 106.8\n",
      "\n",
      "Iter: 592000\n",
      "Loss: 98.25\n",
      "\n",
      "Iter: 593000\n",
      "Loss: 96.94\n",
      "\n",
      "Iter: 594000\n",
      "Loss: 94.76\n",
      "\n",
      "Iter: 595000\n",
      "Loss: 99.58\n",
      "\n",
      "Iter: 596000\n",
      "Loss: 96.39\n",
      "\n",
      "Iter: 597000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 598000\n",
      "Loss: 97.05\n",
      "\n",
      "Iter: 599000\n",
      "Loss: 99.55\n",
      "\n",
      "Iter: 600000\n",
      "Loss: 104.1\n",
      "\n",
      "Iter: 601000\n",
      "Loss: 104.6\n",
      "\n",
      "Iter: 602000\n",
      "Loss: 91.45\n",
      "\n",
      "Iter: 603000\n",
      "Loss: 96.19\n",
      "\n",
      "Iter: 604000\n",
      "Loss: 107.1\n",
      "\n",
      "Iter: 605000\n",
      "Loss: 100.0\n",
      "\n",
      "Iter: 606000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 607000\n",
      "Loss: 88.61\n",
      "\n",
      "Iter: 608000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 609000\n",
      "Loss: 96.44\n",
      "\n",
      "Iter: 610000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 611000\n",
      "Loss: 95.98\n",
      "\n",
      "Iter: 612000\n",
      "Loss: 95.34\n",
      "\n",
      "Iter: 613000\n",
      "Loss: 103.6\n",
      "\n",
      "Iter: 614000\n",
      "Loss: 98.77\n",
      "\n",
      "Iter: 615000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 616000\n",
      "Loss: 102.9\n",
      "\n",
      "Iter: 617000\n",
      "Loss: 99.32\n",
      "\n",
      "Iter: 618000\n",
      "Loss: 96.58\n",
      "\n",
      "Iter: 619000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 620000\n",
      "Loss: 98.31\n",
      "\n",
      "Iter: 621000\n",
      "Loss: 95.62\n",
      "\n",
      "Iter: 622000\n",
      "Loss: 97.59\n",
      "\n",
      "Iter: 623000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 624000\n",
      "Loss: 99.06\n",
      "\n",
      "Iter: 625000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 626000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 627000\n",
      "Loss: 95.73\n",
      "\n",
      "Iter: 628000\n",
      "Loss: 99.84\n",
      "\n",
      "Iter: 629000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 630000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 631000\n",
      "Loss: 95.23\n",
      "\n",
      "Iter: 632000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 633000\n",
      "Loss: 94.22\n",
      "\n",
      "Iter: 634000\n",
      "Loss: 94.32\n",
      "\n",
      "Iter: 635000\n",
      "Loss: 99.5\n",
      "\n",
      "Iter: 636000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 637000\n",
      "Loss: 93.16\n",
      "\n",
      "Iter: 638000\n",
      "Loss: 98.35\n",
      "\n",
      "Iter: 639000\n",
      "Loss: 105.2\n",
      "\n",
      "Iter: 640000\n",
      "Loss: 99.13\n",
      "\n",
      "Iter: 641000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 642000\n",
      "Loss: 92.38\n",
      "\n",
      "Iter: 643000\n",
      "Loss: 93.74\n",
      "\n",
      "Iter: 644000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 645000\n",
      "Loss: 99.4\n",
      "\n",
      "Iter: 646000\n",
      "Loss: 103.3\n",
      "\n",
      "Iter: 647000\n",
      "Loss: 93.28\n",
      "\n",
      "Iter: 648000\n",
      "Loss: 93.8\n",
      "\n",
      "Iter: 649000\n",
      "Loss: 95.88\n",
      "\n",
      "Iter: 650000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 651000\n",
      "Loss: 98.06\n",
      "\n",
      "Iter: 652000\n",
      "Loss: 98.44\n",
      "\n",
      "Iter: 653000\n",
      "Loss: 97.94\n",
      "\n",
      "Iter: 654000\n",
      "Loss: 99.2\n",
      "\n",
      "Iter: 655000\n",
      "Loss: 99.62\n",
      "\n",
      "Iter: 656000\n",
      "Loss: 104.2\n",
      "\n",
      "Iter: 657000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 658000\n",
      "Loss: 97.7\n",
      "\n",
      "Iter: 659000\n",
      "Loss: 96.94\n",
      "\n",
      "Iter: 660000\n",
      "Loss: 99.62\n",
      "\n",
      "Iter: 661000\n",
      "Loss: 99.91\n",
      "\n",
      "Iter: 662000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 663000\n",
      "Loss: 103.7\n",
      "\n",
      "Iter: 664000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 665000\n",
      "Loss: 97.5\n",
      "\n",
      "Iter: 666000\n",
      "Loss: 105.3\n",
      "\n",
      "Iter: 667000\n",
      "Loss: 96.35\n",
      "\n",
      "Iter: 668000\n",
      "Loss: 109.2\n",
      "\n",
      "Iter: 669000\n",
      "Loss: 99.46\n",
      "\n",
      "Iter: 670000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 671000\n",
      "Loss: 101.8\n",
      "\n",
      "Iter: 672000\n",
      "Loss: 97.84\n",
      "\n",
      "Iter: 673000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 674000\n",
      "Loss: 96.1\n",
      "\n",
      "Iter: 675000\n",
      "Loss: 99.65\n",
      "\n",
      "Iter: 676000\n",
      "Loss: 95.16\n",
      "\n",
      "Iter: 677000\n",
      "Loss: 97.56\n",
      "\n",
      "Iter: 678000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 679000\n",
      "Loss: 97.8\n",
      "\n",
      "Iter: 680000\n",
      "Loss: 98.11\n",
      "\n",
      "Iter: 681000\n",
      "Loss: 99.28\n",
      "\n",
      "Iter: 682000\n",
      "Loss: 98.44\n",
      "\n",
      "Iter: 683000\n",
      "Loss: 98.21\n",
      "\n",
      "Iter: 684000\n",
      "Loss: 97.13\n",
      "\n",
      "Iter: 685000\n",
      "Loss: 96.21\n",
      "\n",
      "Iter: 686000\n",
      "Loss: 96.24\n",
      "\n",
      "Iter: 687000\n",
      "Loss: 99.86\n",
      "\n",
      "Iter: 688000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 689000\n",
      "Loss: 106.9\n",
      "\n",
      "Iter: 690000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 691000\n",
      "Loss: 102.7\n",
      "\n",
      "Iter: 692000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 693000\n",
      "Loss: 98.53\n",
      "\n",
      "Iter: 694000\n",
      "Loss: 97.29\n",
      "\n",
      "Iter: 695000\n",
      "Loss: 96.97\n",
      "\n",
      "Iter: 696000\n",
      "Loss: 99.68\n",
      "\n",
      "Iter: 697000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 698000\n",
      "Loss: 98.25\n",
      "\n",
      "Iter: 699000\n",
      "Loss: 97.69\n",
      "\n",
      "Iter: 700000\n",
      "Loss: 93.82\n",
      "\n",
      "Iter: 701000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 702000\n",
      "Loss: 95.34\n",
      "\n",
      "Iter: 703000\n",
      "Loss: 104.8\n",
      "\n",
      "Iter: 704000\n",
      "Loss: 101.1\n",
      "\n",
      "Iter: 705000\n",
      "Loss: 101.8\n",
      "\n",
      "Iter: 706000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 707000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 708000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 709000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 710000\n",
      "Loss: 98.14\n",
      "\n",
      "Iter: 711000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 712000\n",
      "Loss: 97.65\n",
      "\n",
      "Iter: 713000\n",
      "Loss: 105.0\n",
      "\n",
      "Iter: 714000\n",
      "Loss: 94.99\n",
      "\n",
      "Iter: 715000\n",
      "Loss: 96.43\n",
      "\n",
      "Iter: 716000\n",
      "Loss: 99.46\n",
      "\n",
      "Iter: 717000\n",
      "Loss: 96.57\n",
      "\n",
      "Iter: 718000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 719000\n",
      "Loss: 99.85\n",
      "\n",
      "Iter: 720000\n",
      "Loss: 94.74\n",
      "\n",
      "Iter: 721000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 722000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 723000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 724000\n",
      "Loss: 98.51\n",
      "\n",
      "Iter: 725000\n",
      "Loss: 93.73\n",
      "\n",
      "Iter: 726000\n",
      "Loss: 103.4\n",
      "\n",
      "Iter: 727000\n",
      "Loss: 96.52\n",
      "\n",
      "Iter: 728000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 729000\n",
      "Loss: 98.58\n",
      "\n",
      "Iter: 730000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 731000\n",
      "Loss: 107.1\n",
      "\n",
      "Iter: 732000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 733000\n",
      "Loss: 102.9\n",
      "\n",
      "Iter: 734000\n",
      "Loss: 93.18\n",
      "\n",
      "Iter: 735000\n",
      "Loss: 99.16\n",
      "\n",
      "Iter: 736000\n",
      "Loss: 97.67\n",
      "\n",
      "Iter: 737000\n",
      "Loss: 97.54\n",
      "\n",
      "Iter: 738000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 739000\n",
      "Loss: 98.27\n",
      "\n",
      "Iter: 740000\n",
      "Loss: 103.8\n",
      "\n",
      "Iter: 741000\n",
      "Loss: 103.0\n",
      "\n",
      "Iter: 742000\n",
      "Loss: 94.12\n",
      "\n",
      "Iter: 743000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 744000\n",
      "Loss: 97.31\n",
      "\n",
      "Iter: 745000\n",
      "Loss: 99.74\n",
      "\n",
      "Iter: 746000\n",
      "Loss: 103.9\n",
      "\n",
      "Iter: 747000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 748000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 749000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 750000\n",
      "Loss: 93.34\n",
      "\n",
      "Iter: 751000\n",
      "Loss: 93.52\n",
      "\n",
      "Iter: 752000\n",
      "Loss: 99.68\n",
      "\n",
      "Iter: 753000\n",
      "Loss: 99.08\n",
      "\n",
      "Iter: 754000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 755000\n",
      "Loss: 98.71\n",
      "\n",
      "Iter: 756000\n",
      "Loss: 98.92\n",
      "\n",
      "Iter: 757000\n",
      "Loss: 99.3\n",
      "\n",
      "Iter: 758000\n",
      "Loss: 95.56\n",
      "\n",
      "Iter: 759000\n",
      "Loss: 96.06\n",
      "\n",
      "Iter: 760000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 761000\n",
      "Loss: 99.09\n",
      "\n",
      "Iter: 762000\n",
      "Loss: 99.18\n",
      "\n",
      "Iter: 763000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 764000\n",
      "Loss: 103.9\n",
      "\n",
      "Iter: 765000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 766000\n",
      "Loss: 97.88\n",
      "\n",
      "Iter: 767000\n",
      "Loss: 97.49\n",
      "\n",
      "Iter: 768000\n",
      "Loss: 94.88\n",
      "\n",
      "Iter: 769000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 770000\n",
      "Loss: 104.1\n",
      "\n",
      "Iter: 771000\n",
      "Loss: 95.11\n",
      "\n",
      "Iter: 772000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 773000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 774000\n",
      "Loss: 98.61\n",
      "\n",
      "Iter: 775000\n",
      "Loss: 98.69\n",
      "\n",
      "Iter: 776000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 777000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 778000\n",
      "Loss: 97.96\n",
      "\n",
      "Iter: 779000\n",
      "Loss: 98.87\n",
      "\n",
      "Iter: 780000\n",
      "Loss: 102.1\n",
      "\n",
      "Iter: 781000\n",
      "Loss: 96.89\n",
      "\n",
      "Iter: 782000\n",
      "Loss: 104.8\n",
      "\n",
      "Iter: 783000\n",
      "Loss: 96.94\n",
      "\n",
      "Iter: 784000\n",
      "Loss: 95.99\n",
      "\n",
      "Iter: 785000\n",
      "Loss: 99.79\n",
      "\n",
      "Iter: 786000\n",
      "Loss: 94.5\n",
      "\n",
      "Iter: 787000\n",
      "Loss: 98.21\n",
      "\n",
      "Iter: 788000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 789000\n",
      "Loss: 99.77\n",
      "\n",
      "Iter: 790000\n",
      "Loss: 104.2\n",
      "\n",
      "Iter: 791000\n",
      "Loss: 94.07\n",
      "\n",
      "Iter: 792000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 793000\n",
      "Loss: 98.34\n",
      "\n",
      "Iter: 794000\n",
      "Loss: 96.1\n",
      "\n",
      "Iter: 795000\n",
      "Loss: 106.6\n",
      "\n",
      "Iter: 796000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 797000\n",
      "Loss: 96.95\n",
      "\n",
      "Iter: 798000\n",
      "Loss: 97.35\n",
      "\n",
      "Iter: 799000\n",
      "Loss: 94.82\n",
      "\n",
      "Iter: 800000\n",
      "Loss: 95.28\n",
      "\n",
      "Iter: 801000\n",
      "Loss: 96.05\n",
      "\n",
      "Iter: 802000\n",
      "Loss: 94.74\n",
      "\n",
      "Iter: 803000\n",
      "Loss: 97.4\n",
      "\n",
      "Iter: 804000\n",
      "Loss: 94.25\n",
      "\n",
      "Iter: 805000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 806000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 807000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 808000\n",
      "Loss: 98.99\n",
      "\n",
      "Iter: 809000\n",
      "Loss: 98.49\n",
      "\n",
      "Iter: 810000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 811000\n",
      "Loss: 101.8\n",
      "\n",
      "Iter: 812000\n",
      "Loss: 99.9\n",
      "\n",
      "Iter: 813000\n",
      "Loss: 95.98\n",
      "\n",
      "Iter: 814000\n",
      "Loss: 98.92\n",
      "\n",
      "Iter: 815000\n",
      "Loss: 99.37\n",
      "\n",
      "Iter: 816000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 817000\n",
      "Loss: 98.13\n",
      "\n",
      "Iter: 818000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 819000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 820000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 821000\n",
      "Loss: 92.01\n",
      "\n",
      "Iter: 822000\n",
      "Loss: 96.88\n",
      "\n",
      "Iter: 823000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 824000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 825000\n",
      "Loss: 104.2\n",
      "\n",
      "Iter: 826000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 827000\n",
      "Loss: 97.02\n",
      "\n",
      "Iter: 828000\n",
      "Loss: 107.7\n",
      "\n",
      "Iter: 829000\n",
      "Loss: 95.79\n",
      "\n",
      "Iter: 830000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 831000\n",
      "Loss: 98.26\n",
      "\n",
      "Iter: 832000\n",
      "Loss: 103.0\n",
      "\n",
      "Iter: 833000\n",
      "Loss: 98.0\n",
      "\n",
      "Iter: 834000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 835000\n",
      "Loss: 94.95\n",
      "\n",
      "Iter: 836000\n",
      "Loss: 97.63\n",
      "\n",
      "Iter: 837000\n",
      "Loss: 96.22\n",
      "\n",
      "Iter: 838000\n",
      "Loss: 101.0\n",
      "\n",
      "Iter: 839000\n",
      "Loss: 103.5\n",
      "\n",
      "Iter: 840000\n",
      "Loss: 101.2\n",
      "\n",
      "Iter: 841000\n",
      "Loss: 96.67\n",
      "\n",
      "Iter: 842000\n",
      "Loss: 96.12\n",
      "\n",
      "Iter: 843000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 844000\n",
      "Loss: 98.72\n",
      "\n",
      "Iter: 845000\n",
      "Loss: 99.47\n",
      "\n",
      "Iter: 846000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 847000\n",
      "Loss: 96.61\n",
      "\n",
      "Iter: 848000\n",
      "Loss: 97.92\n",
      "\n",
      "Iter: 849000\n",
      "Loss: 108.0\n",
      "\n",
      "Iter: 850000\n",
      "Loss: 104.2\n",
      "\n",
      "Iter: 851000\n",
      "Loss: 98.59\n",
      "\n",
      "Iter: 852000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 853000\n",
      "Loss: 98.63\n",
      "\n",
      "Iter: 854000\n",
      "Loss: 95.02\n",
      "\n",
      "Iter: 855000\n",
      "Loss: 97.84\n",
      "\n",
      "Iter: 856000\n",
      "Loss: 96.04\n",
      "\n",
      "Iter: 857000\n",
      "Loss: 95.99\n",
      "\n",
      "Iter: 858000\n",
      "Loss: 96.94\n",
      "\n",
      "Iter: 859000\n",
      "Loss: 99.39\n",
      "\n",
      "Iter: 860000\n",
      "Loss: 97.03\n",
      "\n",
      "Iter: 861000\n",
      "Loss: 100.9\n",
      "\n",
      "Iter: 862000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 863000\n",
      "Loss: 100.5\n",
      "\n",
      "Iter: 864000\n",
      "Loss: 100.4\n",
      "\n",
      "Iter: 865000\n",
      "Loss: 102.3\n",
      "\n",
      "Iter: 866000\n",
      "Loss: 89.49\n",
      "\n",
      "Iter: 867000\n",
      "Loss: 98.15\n",
      "\n",
      "Iter: 868000\n",
      "Loss: 94.64\n",
      "\n",
      "Iter: 869000\n",
      "Loss: 98.26\n",
      "\n",
      "Iter: 870000\n",
      "Loss: 99.47\n",
      "\n",
      "Iter: 871000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 872000\n",
      "Loss: 99.82\n",
      "\n",
      "Iter: 873000\n",
      "Loss: 100.7\n",
      "\n",
      "Iter: 874000\n",
      "Loss: 99.42\n",
      "\n",
      "Iter: 875000\n",
      "Loss: 103.3\n",
      "\n",
      "Iter: 876000\n",
      "Loss: 91.02\n",
      "\n",
      "Iter: 877000\n",
      "Loss: 98.28\n",
      "\n",
      "Iter: 878000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 879000\n",
      "Loss: 98.97\n",
      "\n",
      "Iter: 880000\n",
      "Loss: 96.16\n",
      "\n",
      "Iter: 881000\n",
      "Loss: 105.5\n",
      "\n",
      "Iter: 882000\n",
      "Loss: 95.77\n",
      "\n",
      "Iter: 883000\n",
      "Loss: 105.7\n",
      "\n",
      "Iter: 884000\n",
      "Loss: 97.15\n",
      "\n",
      "Iter: 885000\n",
      "Loss: 98.32\n",
      "\n",
      "Iter: 886000\n",
      "Loss: 96.69\n",
      "\n",
      "Iter: 887000\n",
      "Loss: 97.43\n",
      "\n",
      "Iter: 888000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 889000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 890000\n",
      "Loss: 94.49\n",
      "\n",
      "Iter: 891000\n",
      "Loss: 100.1\n",
      "\n",
      "Iter: 892000\n",
      "Loss: 99.63\n",
      "\n",
      "Iter: 893000\n",
      "Loss: 96.52\n",
      "\n",
      "Iter: 894000\n",
      "Loss: 94.37\n",
      "\n",
      "Iter: 895000\n",
      "Loss: 93.12\n",
      "\n",
      "Iter: 896000\n",
      "Loss: 95.88\n",
      "\n",
      "Iter: 897000\n",
      "Loss: 99.58\n",
      "\n",
      "Iter: 898000\n",
      "Loss: 96.64\n",
      "\n",
      "Iter: 899000\n",
      "Loss: 95.72\n",
      "\n",
      "Iter: 900000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 901000\n",
      "Loss: 96.41\n",
      "\n",
      "Iter: 902000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 903000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 904000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 905000\n",
      "Loss: 104.0\n",
      "\n",
      "Iter: 906000\n",
      "Loss: 104.0\n",
      "\n",
      "Iter: 907000\n",
      "Loss: 97.5\n",
      "\n",
      "Iter: 908000\n",
      "Loss: 103.8\n",
      "\n",
      "Iter: 909000\n",
      "Loss: 98.03\n",
      "\n",
      "Iter: 910000\n",
      "Loss: 97.3\n",
      "\n",
      "Iter: 911000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 912000\n",
      "Loss: 98.81\n",
      "\n",
      "Iter: 913000\n",
      "Loss: 96.21\n",
      "\n",
      "Iter: 914000\n",
      "Loss: 106.5\n",
      "\n",
      "Iter: 915000\n",
      "Loss: 97.5\n",
      "\n",
      "Iter: 916000\n",
      "Loss: 99.92\n",
      "\n",
      "Iter: 917000\n",
      "Loss: 96.01\n",
      "\n",
      "Iter: 918000\n",
      "Loss: 97.6\n",
      "\n",
      "Iter: 919000\n",
      "Loss: 101.7\n",
      "\n",
      "Iter: 920000\n",
      "Loss: 99.76\n",
      "\n",
      "Iter: 921000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 922000\n",
      "Loss: 96.65\n",
      "\n",
      "Iter: 923000\n",
      "Loss: 93.44\n",
      "\n",
      "Iter: 924000\n",
      "Loss: 92.71\n",
      "\n",
      "Iter: 925000\n",
      "Loss: 105.3\n",
      "\n",
      "Iter: 926000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 927000\n",
      "Loss: 96.8\n",
      "\n",
      "Iter: 928000\n",
      "Loss: 95.14\n",
      "\n",
      "Iter: 929000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 930000\n",
      "Loss: 104.9\n",
      "\n",
      "Iter: 931000\n",
      "Loss: 102.5\n",
      "\n",
      "Iter: 932000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 933000\n",
      "Loss: 96.02\n",
      "\n",
      "Iter: 934000\n",
      "Loss: 105.9\n",
      "\n",
      "Iter: 935000\n",
      "Loss: 94.4\n",
      "\n",
      "Iter: 936000\n",
      "Loss: 99.18\n",
      "\n",
      "Iter: 937000\n",
      "Loss: 99.26\n",
      "\n",
      "Iter: 938000\n",
      "Loss: 99.67\n",
      "\n",
      "Iter: 939000\n",
      "Loss: 96.41\n",
      "\n",
      "Iter: 940000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 941000\n",
      "Loss: 103.2\n",
      "\n",
      "Iter: 942000\n",
      "Loss: 101.4\n",
      "\n",
      "Iter: 943000\n",
      "Loss: 101.9\n",
      "\n",
      "Iter: 944000\n",
      "Loss: 104.3\n",
      "\n",
      "Iter: 945000\n",
      "Loss: 99.12\n",
      "\n",
      "Iter: 946000\n",
      "Loss: 98.82\n",
      "\n",
      "Iter: 947000\n",
      "Loss: 88.76\n",
      "\n",
      "Iter: 948000\n",
      "Loss: 96.98\n",
      "\n",
      "Iter: 949000\n",
      "Loss: 100.8\n",
      "\n",
      "Iter: 950000\n",
      "Loss: 106.4\n",
      "\n",
      "Iter: 951000\n",
      "Loss: 102.2\n",
      "\n",
      "Iter: 952000\n",
      "Loss: 99.25\n",
      "\n",
      "Iter: 953000\n",
      "Loss: 96.69\n",
      "\n",
      "Iter: 954000\n",
      "Loss: 102.0\n",
      "\n",
      "Iter: 955000\n",
      "Loss: 94.83\n",
      "\n",
      "Iter: 956000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 957000\n",
      "Loss: 103.6\n",
      "\n",
      "Iter: 958000\n",
      "Loss: 95.9\n",
      "\n",
      "Iter: 959000\n",
      "Loss: 93.63\n",
      "\n",
      "Iter: 960000\n",
      "Loss: 95.8\n",
      "\n",
      "Iter: 961000\n",
      "Loss: 98.79\n",
      "\n",
      "Iter: 962000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 963000\n",
      "Loss: 99.06\n",
      "\n",
      "Iter: 964000\n",
      "Loss: 103.8\n",
      "\n",
      "Iter: 965000\n",
      "Loss: 104.4\n",
      "\n",
      "Iter: 966000\n",
      "Loss: 96.44\n",
      "\n",
      "Iter: 967000\n",
      "Loss: 102.6\n",
      "\n",
      "Iter: 968000\n",
      "Loss: 102.8\n",
      "\n",
      "Iter: 969000\n",
      "Loss: 98.92\n",
      "\n",
      "Iter: 970000\n",
      "Loss: 99.01\n",
      "\n",
      "Iter: 971000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 972000\n",
      "Loss: 91.83\n",
      "\n",
      "Iter: 973000\n",
      "Loss: 102.4\n",
      "\n",
      "Iter: 974000\n",
      "Loss: 100.2\n",
      "\n",
      "Iter: 975000\n",
      "Loss: 99.41\n",
      "\n",
      "Iter: 976000\n",
      "Loss: 98.5\n",
      "\n",
      "Iter: 977000\n",
      "Loss: 106.7\n",
      "\n",
      "Iter: 978000\n",
      "Loss: 97.22\n",
      "\n",
      "Iter: 979000\n",
      "Loss: 95.37\n",
      "\n",
      "Iter: 980000\n",
      "Loss: 101.3\n",
      "\n",
      "Iter: 981000\n",
      "Loss: 99.78\n",
      "\n",
      "Iter: 982000\n",
      "Loss: 96.19\n",
      "\n",
      "Iter: 983000\n",
      "Loss: 97.69\n",
      "\n",
      "Iter: 984000\n",
      "Loss: 101.5\n",
      "\n",
      "Iter: 985000\n",
      "Loss: 103.1\n",
      "\n",
      "Iter: 986000\n",
      "Loss: 95.29\n",
      "\n",
      "Iter: 987000\n",
      "Loss: 100.6\n",
      "\n",
      "Iter: 988000\n",
      "Loss: 101.6\n",
      "\n",
      "Iter: 989000\n",
      "Loss: 99.65\n",
      "\n",
      "Iter: 990000\n",
      "Loss: 103.0\n",
      "\n",
      "Iter: 991000\n",
      "Loss: 99.68\n",
      "\n",
      "Iter: 992000\n",
      "Loss: 104.0\n",
      "\n",
      "Iter: 993000\n",
      "Loss: 96.52\n",
      "\n",
      "Iter: 994000\n",
      "Loss: 99.26\n",
      "\n",
      "Iter: 995000\n",
      "Loss: 100.3\n",
      "\n",
      "Iter: 996000\n",
      "Loss: 99.28\n",
      "\n",
      "Iter: 997000\n",
      "Loss: 98.96\n",
      "\n",
      "Iter: 998000\n",
      "Loss: 94.5\n",
      "\n",
      "Iter: 999000\n",
      "Loss: 97.34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###generative-model/VAE/conditional_vae\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution() ##解决在tf2版本下使用tf1的API\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "mb_size = 64\n",
    "z_dim = 100 #迭代次数\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128 #隐含层\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# =============================== Q(z|X) ======================================\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "c = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "Q_W1 = tf.Variable(xavier_init([X_dim + y_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))\n",
    "\n",
    "\n",
    "def Q(X, c):\n",
    "    inputs = tf.concat(axis=1, values=[X, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, Q_W1) + Q_b1)\n",
    "    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu\n",
    "    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "# =============================== P(X|z) ======================================\n",
    "\n",
    "P_W1 = tf.Variable(xavier_init([z_dim + y_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))\n",
    "\n",
    "\n",
    "def P(z, c):\n",
    "    inputs = tf.concat(axis=1, values=[z, c])\n",
    "    h = tf.nn.relu(tf.matmul(inputs, P_W1) + P_b1)\n",
    "    logits = tf.matmul(h, P_W2) + P_b2\n",
    "    prob = tf.nn.sigmoid(logits)\n",
    "    return prob, logits\n",
    "\n",
    "\n",
    "# =============================== TRAINING ====================================\n",
    "\n",
    "z_mu, z_logvar = Q(X, c)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "_, logits = P(z_sample, c)\n",
    "\n",
    "# Sampling from random z\n",
    "X_samples, _ = P(z, c)\n",
    "\n",
    "# E[log P(X|z)]\n",
    "recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "# D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "\n",
    "solver = tf.train.AdamOptimizer().minimize(vae_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(1000000):\n",
    "    X_mb, y_mb = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, loss = sess.run([solver, vae_loss], feed_dict={X: X_mb, c: y_mb})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Loss: {:.4}'. format(loss))\n",
    "        print()\n",
    "\n",
    "        y = np.zeros(shape=[16, y_dim])\n",
    "        y[:, np.random.randint(0, y_dim)] = 1.\n",
    "\n",
    "        samples = sess.run(X_samples,\n",
    "                           feed_dict={z: np.random.randn(16, z_dim), c: y})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter-0; Loss: 754.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1000; Loss: 141.9\n",
      "Iter-2000; Loss: 139.5\n",
      "Iter-3000; Loss: 131.6\n",
      "Iter-4000; Loss: 119.8\n",
      "Iter-5000; Loss: 120.5\n",
      "Iter-6000; Loss: 117.1\n",
      "Iter-7000; Loss: 120.0\n",
      "Iter-8000; Loss: 117.0\n",
      "Iter-9000; Loss: 118.6\n",
      "Iter-10000; Loss: 118.0\n",
      "Iter-11000; Loss: 118.5\n",
      "Iter-12000; Loss: 113.4\n",
      "Iter-13000; Loss: 114.8\n",
      "Iter-14000; Loss: 113.6\n",
      "Iter-15000; Loss: 118.1\n",
      "Iter-16000; Loss: 113.7\n",
      "Iter-17000; Loss: 115.0\n",
      "Iter-18000; Loss: 120.0\n",
      "Iter-19000; Loss: 110.3\n",
      "Iter-20000; Loss: 113.6\n",
      "Iter-21000; Loss: 117.5\n",
      "Iter-22000; Loss: 119.8\n",
      "Iter-23000; Loss: 116.2\n",
      "Iter-24000; Loss: 118.5\n",
      "Iter-25000; Loss: 113.5\n",
      "Iter-26000; Loss: 106.0\n",
      "Iter-27000; Loss: 115.5\n",
      "Iter-28000; Loss: 111.2\n",
      "Iter-29000; Loss: 116.6\n",
      "Iter-30000; Loss: 107.3\n",
      "Iter-31000; Loss: 114.2\n",
      "Iter-32000; Loss: 111.3\n",
      "Iter-33000; Loss: 112.6\n",
      "Iter-34000; Loss: 109.7\n",
      "Iter-35000; Loss: 112.9\n",
      "Iter-36000; Loss: 116.6\n",
      "Iter-37000; Loss: 112.6\n",
      "Iter-38000; Loss: 116.8\n",
      "Iter-39000; Loss: 113.4\n",
      "Iter-40000; Loss: 115.5\n",
      "Iter-41000; Loss: 114.4\n",
      "Iter-42000; Loss: 106.9\n",
      "Iter-43000; Loss: 110.7\n",
      "Iter-44000; Loss: 113.8\n",
      "Iter-45000; Loss: 107.9\n",
      "Iter-46000; Loss: 110.7\n",
      "Iter-47000; Loss: 117.7\n",
      "Iter-48000; Loss: 109.3\n",
      "Iter-49000; Loss: 114.3\n",
      "Iter-50000; Loss: 111.3\n",
      "Iter-51000; Loss: 114.8\n",
      "Iter-52000; Loss: 113.4\n",
      "Iter-53000; Loss: 116.9\n",
      "Iter-54000; Loss: 113.6\n",
      "Iter-55000; Loss: 120.0\n",
      "Iter-56000; Loss: 114.5\n",
      "Iter-57000; Loss: 114.1\n",
      "Iter-58000; Loss: 117.3\n",
      "Iter-59000; Loss: 118.2\n",
      "Iter-60000; Loss: 112.7\n",
      "Iter-61000; Loss: 109.1\n",
      "Iter-62000; Loss: 108.7\n",
      "Iter-63000; Loss: 115.9\n",
      "Iter-64000; Loss: 113.7\n",
      "Iter-65000; Loss: 104.5\n",
      "Iter-66000; Loss: 108.6\n",
      "Iter-67000; Loss: 109.8\n",
      "Iter-68000; Loss: 115.5\n",
      "Iter-69000; Loss: 113.5\n",
      "Iter-70000; Loss: 113.2\n",
      "Iter-71000; Loss: 111.2\n",
      "Iter-72000; Loss: 112.0\n",
      "Iter-73000; Loss: 115.0\n",
      "Iter-74000; Loss: 113.7\n",
      "Iter-75000; Loss: 115.0\n",
      "Iter-76000; Loss: 114.3\n",
      "Iter-77000; Loss: 118.7\n",
      "Iter-78000; Loss: 113.6\n",
      "Iter-79000; Loss: 108.6\n",
      "Iter-80000; Loss: 114.1\n",
      "Iter-81000; Loss: 112.8\n",
      "Iter-82000; Loss: 109.9\n",
      "Iter-83000; Loss: 122.3\n",
      "Iter-84000; Loss: 108.5\n",
      "Iter-85000; Loss: 109.7\n",
      "Iter-86000; Loss: 118.5\n",
      "Iter-87000; Loss: 117.9\n",
      "Iter-88000; Loss: 107.0\n",
      "Iter-89000; Loss: 110.2\n",
      "Iter-90000; Loss: 112.5\n",
      "Iter-91000; Loss: 109.2\n",
      "Iter-92000; Loss: 112.6\n",
      "Iter-93000; Loss: 110.1\n",
      "Iter-94000; Loss: 112.4\n",
      "Iter-95000; Loss: 107.6\n",
      "Iter-96000; Loss: 113.9\n",
      "Iter-97000; Loss: 113.2\n",
      "Iter-98000; Loss: 110.8\n",
      "Iter-99000; Loss: 109.9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "noise_factor = .25\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
    "\n",
    "\n",
    "\"\"\" Q(z|X) \"\"\"\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_var\n",
    "\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = Variable(torch.randn(mb_size, Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "\n",
    "\"\"\" P(X|z) \"\"\"\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "\n",
    "def P(z):\n",
    "    h = nn.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
    "          Wzh, bzh, Whx, bhx]\n",
    "\n",
    "solver = optim.Adam(params, lr=lr)\n",
    "\n",
    "for it in range(100000):\n",
    "    X, _ = mnist.train.next_batch(mb_size)\n",
    "    X = Variable(torch.from_numpy(X))\n",
    "\n",
    "    # Add noise\n",
    "    X_noise = X + noise_factor * Variable(torch.randn(X.size()))\n",
    "    X_noise.data.clamp_(0., 1.)\n",
    "\n",
    "    # Forward\n",
    "    z_mu, z_var = Q(X_noise)\n",
    "    z = sample_z(z_mu, z_var)\n",
    "    X_sample = P(z)\n",
    "\n",
    "    torch.nn.BCELoss\n",
    "    recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "    loss = recon_loss + kl_loss\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    solver.step()\n",
    "\n",
    "    # Housekeeping\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; Loss: {:.4}'.format(it, loss.item()))\n",
    "\n",
    "        z = Variable(torch.randn(mb_size, Z_dim))\n",
    "        samples = P(z).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 21:27:36.275624: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-30 21:27:36.280303: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-30 21:27:36.280320: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_90361/4259140303.py:13: read_data_sets (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:296: _maybe_download (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:299: _extract_images (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:304: _extract_labels (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:112: _dense_to_one_hot (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lynn/ENTER/envs/tf/lib/python3.9/site-packages/tensorflow/examples/tutorials/mnist/input_data.py:328: _DataSet.__init__ (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90361/4259140303.py:47: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mb = (mnist.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
      "/tmp/ipykernel_90361/4259140303.py:111: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
      "/tmp/ipykernel_90361/4259140303.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_recon = (generate(H) > 0.5).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "#RBM with Contrastive Divergence (CD)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "\n",
    "mb_size = 16\n",
    "h_dim = 36\n",
    "\n",
    "W = np.random.randn(X_dim, h_dim) * 0.001\n",
    "a = np.random.randn(h_dim) * 0.001\n",
    "b = np.random.randn(X_dim) * 0.001\n",
    "\n",
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def infer(X):\n",
    "    # mb_size x x_dim -> mb_size x h_dim\n",
    "    return sigm(X @ W)\n",
    "\n",
    "\n",
    "def generate(H):\n",
    "    # mb_size x h_dim -> mb_size x x_dim\n",
    "    return sigm(H @ W.T)\n",
    "\n",
    "\n",
    "# Contrastive Divergence\n",
    "# ----------------------\n",
    "# Approximate the log partition gradient Gibbs sampling\n",
    "\n",
    "alpha = 0.1\n",
    "K = 10  # Num. of Gibbs sampling step\n",
    "\n",
    "for t in range(1, 1001):\n",
    "    X_mb = (mnist.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "    g = 0\n",
    "    g_a = 0\n",
    "    g_b = 0\n",
    "\n",
    "    for v in X_mb:\n",
    "        # E[h|v,W]\n",
    "        h = infer(v)\n",
    "\n",
    "        # Gibbs sampling steps\n",
    "        # --------------------\n",
    "        v_prime = np.copy(v)\n",
    "\n",
    "        for k in range(K):\n",
    "            # h ~ p(h|v,W)\n",
    "            h_prime = np.random.binomial(n=1, p=infer(v_prime))\n",
    "            # v ~ p(v|h,W)\n",
    "            v_prime = np.random.binomial(n=1, p=generate(h_prime))\n",
    "\n",
    "        # E[h|v',W]\n",
    "        h_prime = infer(v_prime)\n",
    "\n",
    "        # Compute data gradient\n",
    "        grad_w = np.outer(v, h) - np.outer(v_prime, h_prime)\n",
    "        grad_a = h - h_prime\n",
    "        grad_b = v - v_prime\n",
    "\n",
    "        # Accumulate minibatch gradient\n",
    "        g += grad_w\n",
    "        g_a += grad_a\n",
    "        g_b += grad_b\n",
    "\n",
    "    # Monte carlo gradient\n",
    "    g *= 1 / mb_size\n",
    "    g_a *= 1 / mb_size\n",
    "    g_b *= 1 / mb_size\n",
    "\n",
    "    # Update to maximize\n",
    "    W += alpha * g\n",
    "    a += alpha * g_a\n",
    "    b += alpha * g_b\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# -------------\n",
    "\n",
    "def plot(samples, size, name):\n",
    "    size = int(size)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(size, size), cmap='Greys_r')\n",
    "\n",
    "    plt.savefig('out/{}.png'.format(name), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "\n",
    "H = np.random.binomial(n=1, p=infer(X))\n",
    "plot(H, np.sqrt(h_dim), 'H')\n",
    "\n",
    "X_recon = (generate(H) > 0.5).astype(np.float)\n",
    "plot(X_recon, np.sqrt(X_dim), 'V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90361/1316331946.py:54: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mb = (mnist.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
      "/tmp/ipykernel_90361/1316331946.py:108: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "One layer Binary Helmholtz Machine\n",
    "==================================\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "\n",
    "mb_size = 16\n",
    "h_dim = 36\n",
    "\n",
    "# Recognition/inference weight\n",
    "R = np.random.randn(X_dim, h_dim) * 0.001\n",
    "# Generative weight\n",
    "W = np.random.randn(h_dim, X_dim) * 0.001\n",
    "# Generative bias of hidden variables\n",
    "B = np.random.randn(h_dim) * 0.001\n",
    "\n",
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def infer(X):\n",
    "    # mb_size x x_dim -> mb_size x h_dim\n",
    "    return sigm(X @ R)\n",
    "\n",
    "\n",
    "def generate(H):\n",
    "    # mb_size x h_dim -> mb_size x x_dim\n",
    "    return sigm(H @ W)\n",
    "\n",
    "\n",
    "# Wake-Sleep Algorithm\n",
    "# --------------------\n",
    "alpha = 0.1\n",
    "\n",
    "for t in range(1, 1001):\n",
    "    # ----------\n",
    "    # Wake phase\n",
    "    # ----------\n",
    "\n",
    "    # Upward pass\n",
    "    X_mb = (mnist.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "    H = np.random.binomial(n=1, p=infer(X_mb))\n",
    "\n",
    "    # Downward pass\n",
    "    H_prime = sigm(B)\n",
    "    V = generate(H)\n",
    "\n",
    "    # Compute gradient\n",
    "    dB = H - H_prime\n",
    "    dW = np.array([np.outer(H[i], X_mb[i] - V[i]) for i in range(mb_size)])\n",
    "\n",
    "    # Update generative weight\n",
    "    B += (alpha/t) * np.mean(dB, axis=0)\n",
    "    W += (alpha/t) * np.mean(dW, axis=0)\n",
    "\n",
    "    # -----------\n",
    "    # Sleep phase\n",
    "    # -----------\n",
    "\n",
    "    # Downward pass\n",
    "    H_mb = np.random.binomial(n=1, p=sigm(B))\n",
    "    V = np.random.binomial(n=1, p=generate(H_mb))\n",
    "\n",
    "    # Upward pass\n",
    "    H = infer(V)\n",
    "\n",
    "    # Compute gradient\n",
    "    dR = np.array([np.outer(V, H_mb[i] - H[i]) for i in range(mb_size)])\n",
    "\n",
    "    # Update recognition weight\n",
    "    R += (alpha/t) * np.mean(dR, axis=0)\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# -------------\n",
    "\n",
    "def plot(samples, size, name):\n",
    "    size = int(size)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(size, size), cmap='Greys_r')\n",
    "\n",
    "    plt.savefig('out/{}.png'.format(name), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "\n",
    "H = np.random.binomial(n=1, p=infer(X))\n",
    "plot(H, np.sqrt(h_dim), 'H')\n",
    "\n",
    "X_recon = np.random.binomial(n=1, p=generate(H))\n",
    "plot(X_recon, np.sqrt(X_dim), 'V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90361/3068871243.py:47: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_mb = (mnist.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
      "/tmp/ipykernel_90361/3068871243.py:95: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
      "/tmp/ipykernel_90361/3068871243.py:100: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_recon = (generate(H) > 0.5).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "\n",
    "mb_size = 16\n",
    "h_dim = 36\n",
    "\n",
    "W = np.random.randn(X_dim, h_dim) * 0.001\n",
    "\n",
    "\n",
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def infer(X):\n",
    "    # mb_size x x_dim -> mb_size x h_dim\n",
    "    return sigm(X @ W)\n",
    "\n",
    "\n",
    "def generate(H):\n",
    "    # mb_size x h_dim -> mb_size x x_dim\n",
    "    return sigm(H @ W.T)\n",
    "\n",
    "\n",
    "# Persistent Contrastive Divergence\n",
    "# ---------------------------------\n",
    "# Approximate the log partition gradient using single step Gibbs sampling\n",
    "\n",
    "alpha = 0.1\n",
    "K = 10  # Num. of MC iteration\n",
    "\n",
    "# Initialize the markov chain\n",
    "V_s = sigm(np.random.randn(mb_size, X_dim))\n",
    "H_s = np.random.binomial(n=1, p=0.5, size=[mb_size, h_dim])\n",
    "\n",
    "for t in range(1, 1001):\n",
    "    X_mb = (mnist.train.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "    g = 0\n",
    "\n",
    "    Mu = infer(X_mb)\n",
    "\n",
    "    # Gibbs sampling step\n",
    "    # -------------------\n",
    "    for i, v_s in enumerate(V_s):\n",
    "        for k in range(K):\n",
    "            # h ~ p(h|v,W)\n",
    "            h_prime = np.random.binomial(n=1, p=infer(v_s))\n",
    "            # v ~ p(v|h,W)\n",
    "            v_prime = np.random.binomial(n=1, p=generate(h_prime))\n",
    "\n",
    "        # Replace with new sample\n",
    "        V_s[i] = v_prime\n",
    "        H_s[i] = h_prime\n",
    "\n",
    "    # Compute average gradient\n",
    "    left = np.array([np.outer(X_mb[i], Mu[i]) for i in range(mb_size)])\n",
    "    right = np.array([np.outer(V_s[i], H_s[i]) for i in range(mb_size)])\n",
    "    g = np.mean(left, axis=0) - np.mean(right, axis=0)\n",
    "\n",
    "    # Update\n",
    "    W += alpha * g  # Maximize likelihood\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# -------------\n",
    "\n",
    "def plot(samples, size, name):\n",
    "    size = int(size)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(size, size), cmap='Greys_r')\n",
    "\n",
    "    plt.savefig('out/{}.png'.format(name), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n",
    "\n",
    "H = np.random.binomial(n=1, p=infer(X))\n",
    "plot(H, np.sqrt(h_dim), 'H')\n",
    "\n",
    "X_recon = (generate(H) > 0.5).astype(np.float)\n",
    "plot(X_recon, np.sqrt(X_dim), 'V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 10:46:03.517610: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-31 10:46:03.522333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/gcc-9.3.0/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/usr/local/gcc-9.3.0/lib64:/usr/local/gcc-9.3.0/lib/gcc/x86_64-pc-linux-gnu/9.3.0/plugin::/usr/local/lib:/usr/local/cuda-11.4/lib64\n",
      "2022-07-31 10:46:03.522351: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../MNIST_data', one_hot=True)\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "\n",
    "mb_size = 16\n",
    "h_dim = 36\n",
    "\n",
    "W = np.random.randn(X_dim, h_dim) * 0.001 #(784, 36)\n",
    "a = np.random.randn(h_dim) * 0.001 #(36,)\n",
    "b = np.random.randn(X_dim) * 0.001 #(784,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def infer(X):\n",
    "    # mb_size x x_dim -> mb_size x h_dim\n",
    "    return sigm(X @ W) #(16,36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45418/1936075023.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "X = (mnist.test.next_batch(mb_size)[0] > 0.5).astype(np.float)  #(16,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 36)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(n=1, p=infer(X)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 36)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(X).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5435f4404a8fbe85e940139feda3d395d467c89d810cfc8ebe53f500e502041"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
